{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobility Data Analysis\n",
    "1. Scoping\n",
    "2.  Preprocress the data\n",
    "3. Explolatory Data Analysis (EDA)\n",
    "4. Generate user trips and other user attributes\n",
    "5. Perfom analysis of user data to generate individual level metrics\n",
    "6. Generate aggregate metrics such as OD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Scoping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "1. Please fill out all the places with \"YOUR CODE HERE\" with your code.\n",
    "2. Fill in with your functions, in places where I ask you to do so.\n",
    "3. Please answer all non-code questions where "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Python setup\n",
    "Heree, we import all the required Python packages. In order to use, \n",
    "any other module which wasnt ```pip``` installed, such as ```mob_data_utils```,\n",
    "you  can do the following:\n",
    "```sys.append(full_path_to_module)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# utility libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from functools import wraps\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Apache Spark Modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import col,udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# plotting library\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=1.25, rc={\"lines.linewidth\":1.25, \"lines.markersize\":8})\n",
    "\n",
    "# local libraries (e.g., mob_data_utils)\n",
    "# since mob_data_utils.py is in this dir\n",
    "import mob_data_utils as ut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup working directories\n",
    "Its also important to setup commonly used diretories such as where you will be saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use CAPS for these variables since they are constants\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR.joinpath('data')\n",
    "OUTPUTS_DIR = BASE_DIR.joinpath('outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup global parameters and variables\n",
    "MISC_PROCESSING_PARAMS = {'distance_threshold': 2, 'min_unique_locs': 2,'datetime_col': 'datetime',\n",
    "                        'userid': 'user_id','x': 'lon', 'y': 'lat'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing.\n",
    "Often, after all the data has been  acquired, thee next step is to do some preprocessing on the raw data. \n",
    "The objectives of this task will vary depending on the data analysis goals but some of them include following:\n",
    "- **Sanitize the data:** this data cleaning has to be done carefully to avoid introducing errors but its often a necessary step. It can involve dropping some unnecessary variables/columns. Renaming some columns to something which makes more sense. Dropping some observations. For instance, in this analysis where location and time-stamp is important, dropping all observations with no time-stamp and no location is required.\n",
    "- **Create new variables:**. If necessary, this is also the time transform some variables from a format which is not convinient for your analysis. For instance, converting string time variables to datetime aware variables.\n",
    "- **Combine datasets:** If you have more than one dataset, during preprocessing, you can also combine several datasets into one. For instance, we have the CDR transations which have no location details. We bring in the location details from another file.\n",
    "- **Filtering based on columns and observations:** This can be done through any of the stages mentioned above but its worth mentioning that often, you may drop some columns which arent useful for your analysis. Also, you may drop some observations based on some conditions depending on your analysis needs.\n",
    "\n",
    "Unlike in other data collection domains such as surveys where you can have standard data processing steps, in the data science space where your dataset can be anything, there are no hard and fast rule for preprocessing and data cleaning. It will be a case by case basis depending on your analysis goals. Also, preprocessing isnt necessarily a linear process: depending on what results you get downstream, you can go back and modify the preprocesisng steps. In this project, we have the ```preprocess_cdrs_using_spark``` which takes raw cdrs and saves to a CSV a processed dataset. Alternatively, we can return a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def rename_sdf(df, mapper=None):\n",
    "    ''' Rename column names of a dataframe\n",
    "        mapper: a dict mapping from the old column names to new names\n",
    "        Usage:\n",
    "            df.rename({'old_col_name': 'new_col_name', 'old_col_name2': 'new_col_name2'})\n",
    "            df.rename(old_col_name=new_col_name)\n",
    "    '''\n",
    "    for before, after in mapper.items():\n",
    "        df = df.withColumnRenamed(before, after)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# ADD YOUR PROCESS FUNCTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DATA_DIR and joinpath as its been used above to create\n",
    "# full path for simulated_cdrs and loc file\n",
    "loc_file = YOUR CODE\n",
    "cdrs_dir = YOUR CODE\n",
    "num_users = YOUR CODE\n",
    "debug = True\n",
    "\n",
    "# call preprocess_cdrs_using_spark here\n",
    "# use cache() at the end of the like this preprocess_cdrs_using_spark.cache()\n",
    "# Learn about what cache does using spark here:\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.cache.html#pyspark.sql.DataFrame.cache\n",
    "dfu = preprocess_cdrs_using_spark(file_or_folder=str(cdrs_dir), number_of_users_to_sample=num_users,\n",
    "                                date_format='%Y%m%d%H%M%S',debug_mode=False, \n",
    "                                  loc_file=loc_file, save_to_csv=False).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explolatory Data Analysis (EDA)\n",
    "Whether the end result of your project is to produce a statistical report or \n",
    "to build a prediction model to be put in production, EDA is an essential stage in any data science project. EDA can be defined as \n",
    "the process of performing initial investigations on data so as to discover patterns,to spot anomalies,\n",
    "to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.\n",
    "It is a good practice to understand the data first and try to gather as many insights from it. \n",
    "EDA is all about making sense of data before using the data for the intended use (e.g., build ML models, perfom statisitcal analysis). \n",
    "\n",
    "Again, there arent hard and fast rules on how to perfom EDA but some of the specific quesitons you would like to answer are as folloes:\n",
    "- For each variable in the data, whats its distribution? Is it skewed? Whats its data type? Is it an approapriate  data type fopr my analysis. Are there any outliers?\n",
    "- Whats the relationship between variables?\n",
    "\n",
    "In this project, we will use the ```explore_data``` and explore what functions SPark has for basic EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR EXPLORE DATA FUNC HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate more summary statistics\n",
    "Please complete the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def summary_stats_for_user_events(spark_df, out_stats):\n",
    "    \"\"\"\n",
    "    In this function, the goal is to take a big Spark\n",
    "    DataFrame, group users and count each users events, \n",
    "    convert to pandas DataFrame and generate summary stats\n",
    "    :param: spark_df: preprocessed spark dataframe with data for multiple users\n",
    "    :param: out_stats: CSV file path to save  the summary stats\n",
    "    \"\"\"\n",
    "    # group user and count number of events\n",
    "    # convert resulting spark dataframe to pandas\n",
    "    pdf = YOUR CODE\n",
    "    # change column \"count\" to num_events,remember that pdf is a pandas DataFrame\n",
    "    YOUR CODE\n",
    "    \n",
    "    # generate summary stats using pandas describe() function\n",
    "    # use property T to transpose the describe results and convert them\n",
    "    # into a DataFrame like this: pd.DataFrame(transposed describe results).reset_index()\n",
    "    # Dont forget to rese_index()\n",
    "    pdf_sum_stats = YOUR CODE\n",
    "    \n",
    "    # remove the first row which has value \"count\"\n",
    "    # you can use list indexing to achieve this\n",
    "    pdf_sum_stats = YOUR CODE\n",
    "    \n",
    "    # Rename the column index into something informative. For instance, \"Stat\"\n",
    "    YOUR CODE\n",
    "    \n",
    "    # Rename the percentiles in numbers to something better\n",
    "    # first, declare a dict with old and new names\n",
    "    # next, update the Stats column using the pd.Series.map() function\n",
    "    YOUR CODE\n",
    "    YOUR CODE\n",
    "    \n",
    "    print(*)\n",
    "    # Now  save the summary stats to CSV\n",
    "    YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALL YOUR summary_stats_for_user_events FUNCTION HERE, \n",
    "# REMEMBER THAT IT REQUIRES THE PREPROCESSED DF FROM \n",
    "# preprocess_cdrs_using_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Interpreting EDA results and applying them in downstreat work\n",
    "Please answer the question below. You should answer in this same notebook.\n",
    "\n",
    "**EDA-Question-1**: Given the distribution of number of events per user. When it comes to characterizing user mobility patterns such as number  of trips. Do you think we should utilize all users regardless of number of events?\n",
    "\n",
    "A. Yes, we can use all users regardless of number of events.\n",
    "\n",
    "B. No, we should filter out some users\n",
    "\n",
    "**Your answer:**\n",
    "\n",
    "**EDA-Question-2**: If you answered B, please answer the following questions on filtering?\n",
    "1. How do determine threshold for filtering?\n",
    "2. Which users do you filter out? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate individual based mobility patterns and attributes\n",
    "As we noted in the project  instructions, the focus of this analysis is to understand mobility patterns \n",
    "on individual users. Although, generating trips  and understanding their distribution is crucial for this project, due to time constraints, we will start with simple mobility metrics. Namely:\n",
    "- **Radius of gyration(Rg):**  For a single day, Rg can be defined in simple terms as the maximum distance a user travels. We can then compute ```avg_Rg``` based on all Rg from the user's data. This metric ```avg_Rg``` is what we will compute.\n",
    "- **Number of unique locations visited everyday:** As the name suggests, this is simply, the count of unique locations an individual visits everyday. Given multiple days data, we will compute the ```avg_locs_per_day```\n",
    "\n",
    "In addition to the mobility metric above, we will report the ```number of days``` a user was activive which will help us understand how much we should trust user data. \n",
    "\n",
    "For this task, we will utilize functions in the ```mob_data_utils``` module which were already created to generate  the required metrics above. You can import ```mob_data_utils``` like this to use my code: \n",
    "```import mob_data_utils as ut```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to generate user attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_basic_user_mob_attributes(df):\n",
    "        \"\"\"\n",
    "        In this funciton, we generate some basic user attributes \n",
    "        to help further explore the data and also report on \n",
    "        individual mobility metrics.\n",
    "        :param df: Pandas DataFrame of single user data\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # get, datecol, x(lon), y(lat) from the MISC_PROCESSING_PARAMS variable\n",
    "        datetimecol = YOUR CODE\n",
    "        x =  YOUR CODE\n",
    "        y =  YOUR CODE\n",
    "        \n",
    "        # use if condition to \n",
    "        if  'date' not in df.columns:\n",
    "            # add date column in case its not  there\n",
    "            # use the datetimecol to achieve this\n",
    "            df['date'] = YOUR CODE\n",
    "        \n",
    "        # get a list of all  days/dates in ascending order\n",
    "        # first, sort the dates and then get only unique dates\n",
    "        dates = YOUR CODE\n",
    "        # this dictionary will keep, for each, a count of unique locations visited\n",
    "        # initialize this dict with dates as keys and values set to 0\n",
    "        # you can use list comprehension idea though this is a dictionary\n",
    "        # Hint, create a list  of unique dates when initializing this dict\n",
    "        unique_locs_by_day = YOUR CODE\n",
    "        \n",
    "        # create a dictionary just like above but this one will keep\n",
    "        # maximum distance travelled for each day\n",
    "        dates_dist = YOUR CODE \n",
    "        \n",
    "        # Loop through the dates_dist dictionary\n",
    "        YOUR CODE \n",
    "            # Filter the input df so that we only get data for this date\n",
    "            dfd = YOUR CODE \n",
    "\n",
    "            # get number of unique locations for this day\n",
    "            uniq_xy = ut.va_generate_unique_locs(df=dfd, x=x, y=y)\n",
    "            # add to the unique_locs_by_day dict, this date based on how\n",
    "            # you initialized your loop, the value is the number of\n",
    "            # unique locations visited\n",
    "            YOUR CODE\n",
    "\n",
    "\n",
    "            # distances travel\n",
    "            if len(uniq_xy) > 1:\n",
    "                dist_mtx = ut.va_distance_matrix(uniq_xy)\n",
    "                # From the distance matrix above, get only columns with \"to\"\n",
    "                # in it, use list comprehension with if condition\n",
    "                req_cols = YOUR CODE\n",
    "                # get max value from the distance matrix above\n",
    "                # first, subset the dist_mtx DataDrame by selecting only req_cols\n",
    "                # then, get values from the resulting DataFrame which you should\n",
    "                # pass into np.max() function\n",
    "                # put the resulting max value into the dates_dist dict with this \n",
    "                # date as key\n",
    "                # this can be achieved in a single line of code or multiple lines\n",
    "                YOUR CODE\n",
    "            else:\n",
    "                # if number of unique locations is less than or equal to 1\n",
    "                # then set the value in the dates_dist dict accordingly\n",
    "                 YOUR CODE\n",
    "        \n",
    "        # return dates_dist, unique_locs_by_day, number of days\n",
    "        YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def generate_basic_user_attributes_with_pandas(df, outcsv, num_events_threshold=None):\n",
    "    \"\"\"\n",
    "        In this funciton, we generate some basic user attributes \n",
    "        to help further explore the data and also report on \n",
    "        individual mobility metrics.\n",
    "        :param df: Pandas DataFrame with multiple user data\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    # get userid col name from  MISC_PROCESSING_PARAMS\n",
    "    userid = YOUR CODE\n",
    "    # generate a list of unique userid's\n",
    "    user_list = YOUR CODE\n",
    "    # initialize an empty list to hold user  data\n",
    "    user_data = YOUR CODE\n",
    "    \n",
    "    # Loop through all users and generate their attributes\n",
    "    YOUR CODE\n",
    "        # Filter the input df so that we only get data for this user\n",
    "        df_user = YOUR CODE\n",
    "        \n",
    "        if num_events_threshold:\n",
    "            if df_user.shape[0] < num_events_threshold:\n",
    "                continue\n",
    "            else:\n",
    "                # call the get_basic_user_mob_attributes function here\n",
    "                YOUR CODE\n",
    "        else:\n",
    "             # call the get_basic_user_mob_attributes function here\n",
    "                YOUR CODE\n",
    "        \n",
    "        # get the attributes\n",
    "        # create a dictionary with the following keys: 'userid', 'usage_days', 'mean_locs_day'\n",
    "        # use appropriate numpy functions to compute  mean as required and set them as values\n",
    "        # in the dict\n",
    "        user_att = YOUR CODE\n",
    "        # add user_att to the user_data list\n",
    "        YOUR CODE\n",
    "    \n",
    "    # create DataFrame using user_data and save it to file (2 lines of code)\n",
    "    YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALL YOUR generate_basic_user_attributes_with_pandas() FUNCTION HERE, \n",
    "# FIRST, CONVERT THE DF FROM preprocess_cdrs_using_spark() TO PANDAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create outputs from the CSV file of user attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION-PART1: DEFINE A FUNCTION WITH FOLLOWING PROPERTIES:\n",
    "- **Function name:** generate_outputs_from_csv()\n",
    "- **Inputs:** csv file which you save from generate_basic_user_attributes_with_pandas function above\n",
    "- **Inside the function:** use any plotting library (e.g., seaborn as we have used in this course) to generate a distribution plot of ```Radius of gyration(Rg)```.  Make sure your function shows the plot inline when it runs.\n",
    "- **Function output:** The function doesnt have to return anything\n",
    "\n",
    "### QUESTION-PART2: INTERPRET THE RESULTS\n",
    "Write a few sentences to interpret the plot ```Radius of gyration(Rg)``` that you generated. Based on how it looks, do you think its normally distributed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION: EXPLORE RELATIONSHIP BETWEEN ```avg_Rg``` and ```avg_locs_per_day```\n",
    "Using the CSV that you saved above, perfom analyis (you dont have to use a function)\n",
    "- Use a relevant plot to show relationship between the two variables\n",
    "- Report the correlation coefficient between the two variables\n",
    "- Write a few sentences to interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
