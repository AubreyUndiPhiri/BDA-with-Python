{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d99f5ec-10e5-451e-9a01-742d7df537ab",
   "metadata": {},
   "source": [
    "# ML with large datasets\n",
    "In this tutorial, we'll explore what happens when a dataset becomes large as we \n",
    "train a machine learning model with standard packages such as sklearn. We will do the following:\n",
    "1. Investigate model training running time on a ```4-GB``` dataset while we modify the input number of rows and number of cores being run on.\n",
    "2. Explore how to build a simple ML model using sparkMLlib\n",
    "\n",
    "We will use the dataset from Kaggl Expedia competition. Please read about the competition, lear about whats being predicted and more from [here](https://www.kaggle.com/c/expedia-hotel-recommendations). You can log into Kaggle with your Google account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb76f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier  as RF\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a561fb-5a54-46af-8197-dff2c1949c26",
   "metadata": {},
   "source": [
    "## Investigate model training time in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02ef8c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def map5eval(preds, dtrain):\n",
    "    actual = dtrain.get_label()\n",
    "    predicted = preds.argsort(axis=1)[:,-np.arange(5)]\n",
    "    metric = 0.\n",
    "    for i in range(5):\n",
    "        metric += np.sum(actual==predicted[:,i])/(i+1)\n",
    "    metric /= actual.shape[0]\n",
    "    return 'MAP@5', -metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86442773",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"../DATA/kaggle-expedia-train.csv\"\n",
    "COLS = ['site_name', 'user_location_region', 'is_package', 'srch_adults_cnt', 'srch_children_cnt','srch_destination_id', 'hotel_market', 'hotel_country', 'hotel_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ad6df-d5d7-489c-9bff-679bf2cb3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_FILE, nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826bd06-46d3-45d7-8612-be6e68d30b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hotel_cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f341218",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def read_pandas_df(num_rows=100000, chunk_size=100000):\n",
    "    \n",
    "    tot_rows = 0\n",
    "    df_train = pd.DataFrame(columns=COLS)\n",
    "    train_chunk = pd.read_csv(DATA_FILE, chunksize=chunk_size)\n",
    "    i = 0\n",
    "    for chunk in train_chunk:\n",
    "        df_train = pd.concat([df_train, chunk[chunk['is_booking'] == 1][COLS]])\n",
    "        tot_rows += df_train.shape[0]\n",
    "        i = i + 1\n",
    "#         if i % 10 == 0:\n",
    "#             print(\"Rows loaded: \" + str(i / 10) + \"mill\")\n",
    "        \n",
    "#         if (num_rows - tot_rows) < 1000:\n",
    "#             break\n",
    "    \n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13ae4010",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def build_RF_with_sklearn(df):\n",
    "    for column in df:\n",
    "        df[column] = df[column].astype(str).astype(int)\n",
    "\n",
    "    # print(df_train.shape())\n",
    "    X = df.drop(['hotel_cluster'],axis=1)\n",
    "    y = df['hotel_cluster'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    start = datetime.datetime.now()\n",
    "    clf = RF(n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    end = datetime.datetime.now()\n",
    "    time_taken = (end-start).total_seconds()/60\n",
    "    num_rows = X.shape[0]\n",
    "    print('Training {:,} rows took {} minutes with all cores'.format(num_rows, time_taken))\n",
    "    return time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "701fab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [i*1e6 for i in range(5,20)]\n",
    "data = []\n",
    "current_nrows = 0\n",
    "for r in rows:\n",
    "    try:\n",
    "        df = read_pandas_df(num_rows=r, chunk_size=2000000)\n",
    "        time_taken = build_RF_with_sklearn(df)\n",
    "        pandas_outputs.append({'TimeTaken': round(time_taken, 4), 'NumRows': df.shape[0]})\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d97b4-934e-470a-8e6c-9f2be5e2c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_pandas_df(num_rows=5000000, chunk_size=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c1775e-526b-4e45-ac59-6ede32573d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee525c-5b9e-4616-833f-4e3964dad379",
   "metadata": {},
   "source": [
    "## Explore sparkMLlib API\n",
    "We will follow this simple tutorial from [sparkMLlib](https://spark.apache.org/docs/1.2.1/mllib-guide.html) page before using our own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172d560-34d6-4e74-aebd-85a38a5d76af",
   "metadata": {},
   "source": [
    "### Data structures\n",
    "The bad news is that you often have to transform your dataset to match with data \n",
    "structured required by MLlib. The way that features and labels are represented in spark-MLLib is \n",
    "different from how its done in sklearn. In spark, they use the class, \n",
    "LabeledPoint, which takes both dense and sparse feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294874d7-55d4-49ed-97fa-10b2113ab664",
   "metadata": {},
   "source": [
    "**EXERCISE-1:** Learn more about LabeledPoint, dense and sparse feature vectors.\n",
    "Use MLlib documentation page to learn more about these data structures.\n",
    "- How do they differ from regular Python Numpy arrays which are used in sklelarn and pandas?\n",
    "- How do you create them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72a00a5d-9202-4011-a914-52f90665be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a labeled point with a positive label and a dense feature vector.\n",
    "pos = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n",
    "\n",
    "# Create a labeled point with a negative label and a sparse feature vector.\n",
    "neg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb7cb0c-4b48-4e70-a8fe-44cd9b3b1ee2",
   "metadata": {},
   "source": [
    "### Build simple model using RDD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52c0caf9-00e4-4829-b56a-0a9c05b594d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsePoint(line):\n",
    "    \"\"\"\n",
    "    Convert row into a LabeledPoint data type as required by spark\n",
    "    \"\"\"\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2c568d1-799a-4e44-ae6a-364a4bf814ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 08:32:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/01/19 08:32:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"ML-app\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "data_rdd = sc.textFile(\"../DATA/sample_svm_data.txt\")\n",
    "# parsedData = data_rdd.map(parsePoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e28996f-0220-4019-bc95-a1eed93e527b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1 0 2.52078447201548 0 0 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e47ac-e6ea-4b47-934b-d516481c1617",
   "metadata": {},
   "source": [
    "**EXERCISE-2:** Convert this RDD to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5d495fc-4c33-4a7f-86b4-c9bc789dd4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.5\n",
      "Learned classification forest model:\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 3 <= 1.3099825520441275)\n",
      "     If (feature 14 <= 6.364083791088865)\n",
      "      If (feature 11 <= 1.027501437932207)\n",
      "       If (feature 0 <= 1.428869016623521)\n",
      "        Predict: 1.0\n",
      "       Else (feature 0 > 1.428869016623521)\n",
      "        Predict: 0.0\n",
      "      Else (feature 11 > 1.027501437932207)\n",
      "       Predict: 0.0\n",
      "     Else (feature 14 > 6.364083791088865)\n",
      "      Predict: 1.0\n",
      "    Else (feature 3 > 1.3099825520441275)\n",
      "     If (feature 1 <= 1.26039223600774)\n",
      "      If (feature 11 <= 1.027501437932207)\n",
      "       Predict: 1.0\n",
      "      Else (feature 11 > 1.027501437932207)\n",
      "       If (feature 4 <= 2.372526427751653)\n",
      "        Predict: 1.0\n",
      "       Else (feature 4 > 2.372526427751653)\n",
      "        Predict: 0.0\n",
      "     Else (feature 1 > 1.26039223600774)\n",
      "      Predict: 1.0\n",
      "  Tree 1:\n",
      "    If (feature 3 <= 1.3099825520441275)\n",
      "     If (feature 9 <= 1.1141935213710115)\n",
      "      Predict: 0.0\n",
      "     Else (feature 9 > 1.1141935213710115)\n",
      "      If (feature 11 <= 1.027501437932207)\n",
      "       If (feature 1 <= 1.26039223600774)\n",
      "        Predict: 0.0\n",
      "       Else (feature 1 > 1.26039223600774)\n",
      "        Predict: 1.0\n",
      "      Else (feature 11 > 1.027501437932207)\n",
      "       Predict: 0.0\n",
      "    Else (feature 3 > 1.3099825520441275)\n",
      "     If (feature 7 <= 1.0614871893948106)\n",
      "      If (feature 4 <= 2.372526427751653)\n",
      "       If (feature 1 <= 1.26039223600774)\n",
      "        Predict: 1.0\n",
      "       Else (feature 1 > 1.26039223600774)\n",
      "        Predict: 0.0\n",
      "      Else (feature 4 > 2.372526427751653)\n",
      "       Predict: 0.0\n",
      "     Else (feature 7 > 1.0614871893948106)\n",
      "      Predict: 0.0\n",
      "  Tree 2:\n",
      "    If (feature 3 <= 1.3099825520441275)\n",
      "     Predict: 0.0\n",
      "    Else (feature 3 > 1.3099825520441275)\n",
      "     If (feature 6 <= 1.000173649634233)\n",
      "      If (feature 0 <= 1.428869016623521)\n",
      "       If (feature 5 <= 1.002342218247152)\n",
      "        Predict: 0.0\n",
      "       Else (feature 5 > 1.002342218247152)\n",
      "        Predict: 1.0\n",
      "      Else (feature 0 > 1.428869016623521)\n",
      "       Predict: 1.0\n",
      "     Else (feature 6 > 1.000173649634233)\n",
      "      If (feature 1 <= 1.26039223600774)\n",
      "       If (feature 4 <= 2.372526427751653)\n",
      "        Predict: 1.0\n",
      "       Else (feature 4 > 2.372526427751653)\n",
      "        Predict: 0.0\n",
      "      Else (feature 1 > 1.26039223600774)\n",
      "       If (feature 5 <= 1.002342218247152)\n",
      "        Predict: 0.0\n",
      "       Else (feature 5 > 1.002342218247152)\n",
      "        Predict: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "data = data_rdd.map(parsePoint)\n",
    "# data = MLUtils.loadLibSVMFile(sc, '../DATA/sample_libsvm_data.txt')\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification forest model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "model_output = \"../MODELS/spark-models/spark-RF2\"\n",
    "model.save(sc, model_output)\n",
    "sameModel = RandomForestModel.load(sc, model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "185c9dbb-96f3-4efd-a57f-51c0523601e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.5\n"
     ]
    }
   ],
   "source": [
    "predictions = sameModel.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d2d22-ff27-4c00-bb3c-8f7340f7517c",
   "metadata": {},
   "source": [
    "### Build model with our own custom dataset\n",
    "We will use the expedia dataset for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5e7c5",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model_with_sparkMLlib(input_file, sample_size=0.5):\n",
    "    \n",
    "    # Load and parse the data file, converting it to a DataFrame.\n",
    "    spark = SparkSession.builder.master(\"local[*]\").appName(\"ML-app\").getOrCreate()\n",
    "    schema = \"`user_location_region` INT, `is_package` INT,`srch_adults_cnt` INT, `srch_children_cnt` INT,`srch_destination_id` INT, `hotel_market` INT, `hotel_country` INT,`hotel_cluster` INT\"\n",
    "    sdf = spark.read.schema(schema).csv(input_file)\n",
    "    sdf = sdf.dropna()\n",
    "    sdf_sample = sdf.sample(fraction=0.5).cache()\n",
    "    nrows = sdf_sample.count()\n",
    "    \n",
    "    \n",
    "    feature_cols = list(set(sdf.columns)-set(['hotel_cluster']))\n",
    "    print(feature_cols)\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "    transformed_train_data = assembler.transform(sdf_sample).cache()\n",
    "    \n",
    "    (train, test) = transformed_train_data.randomSplit([0.8, 0.2])\n",
    "    \n",
    "    rf = RandomForestClassifier(labelCol='hotel_cluster', featuresCol='features')\n",
    "    rf.fit(train)\n",
    "    \n",
    "    print('Done fitting model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11e215d3-d823-4168-9a4a-7fe58e3eee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"ML-app\").getOrCreate()\n",
    "schema = \"`user_location_region` INT, `is_package` INT,`srch_adults_cnt` INT, `srch_children_cnt` INT,`srch_destination_id` INT, `hotel_market` INT, `hotel_country` INT,`hotel_cluster` INT\"\n",
    "sdf = spark.read.schema(schema).csv(DATA_FILE)\n",
    "sdf_sample = sdf.sample(fraction=0.5).cache()\n",
    "#nrows = sdf_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "823c474b-a303-46c5-8e92-88e2ebeca0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=============================================>          (25 + 6) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------------+-----------------+-------------------+------------+-------------+-------------+\n",
      "|user_location_region|is_package|srch_adults_cnt|srch_children_cnt|srch_destination_id|hotel_market|hotel_country|hotel_cluster|\n",
      "+--------------------+----------+---------------+-----------------+-------------------+------------+-------------+-------------+\n",
      "|            18827216|         0|              0|                0|                  0|           0|     18827216|            0|\n",
      "+--------------------+----------+---------------+-----------------+-------------------+------------+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check nulls\n",
    "sdf_sample.select([count(when(isnull(c), c)).alias(c) for c in sdf_sample.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15d01a7e-ad06-446e-a60d-2bbfac2c2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nulls\n",
    "cols_to_drop = ['user_location_region', 'hotel_country']\n",
    "sdf_sample2 = sdf_sample.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b56db2a8-54cc-4e49-a965-60fd1cf37f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sample3 = sdf_sample2.dropna(how='any')\n",
    "#sdf_sample3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae39f5-49f3-43d2-ab82-a14966743738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelIndexer = StringIndexer(inputCol=\"hotel_cluster\", outputCol=\"indexedLabel\").fit(sdf_sample3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3b1c87-bb89-4ebe-b71e-b01c9b4ea24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sample3.select([count(when(isnull(c), c)).alias(c) for c in sdf_sample3.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef20bfba-197b-4448-9c50-517b7516f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = list(set(sdf_sample3.columns)-set(['hotel_cluster']))\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "transformed_train_data = assembler.transform(sdf_sample3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c484546-21ec-4e7e-878a-9e221f6c4b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----------------+-------------------+------------+-------------+--------------------+\n",
      "|is_package|srch_adults_cnt|srch_children_cnt|srch_destination_id|hotel_market|hotel_cluster|            features|\n",
      "+----------+---------------+-----------------+-------------------+------------+-------------+--------------------+\n",
      "|         2|              3|               66|                348|       48862|           12|[348.0,3.0,66.0,2...|\n",
      "|         2|              3|               66|                348|       48862|           12|[348.0,3.0,66.0,2...|\n",
      "|         2|              3|               66|                442|       35390|           93|[442.0,3.0,66.0,2...|\n",
      "|         2|              3|               66|                189|       10067|          501|[189.0,3.0,66.0,2...|\n",
      "|         2|              3|               66|                189|       10067|          501|[189.0,3.0,66.0,2...|\n",
      "+----------+---------------+-----------------+-------------------+------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30e0a0de-3cb9-4c94-bf5f-960e20e8c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = transformed_train_data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a78c9319-9506-42e1-8341-79d26c3c6d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:=============================================>          (25 + 6) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 08:46:42 ERROR Instrumentation: java.lang.IllegalArgumentException: requirement failed: Classifier inferred 1198786 from label values in column RandomForestClassifier_9a220eeb559c__labelCol, but this exceeded the max numClasses (100) allowed to be inferred from values.  To avoid this error for labels with > 100 classes, specify numClasses explicitly in the metadata; this can be done by applying StringIndexer to the label column.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.classification.Classifier.getNumClasses(Classifier.scala:157)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:143)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Classifier inferred 1198786 from label values in column RandomForestClassifier_9a220eeb559c__labelCol, but this exceeded the max numClasses (100) allowed to be inferred from values.  To avoid this error for labels with > 100 classes, specify numClasses explicitly in the metadata; this can be done by applying StringIndexer to the label column.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w3/dfvgjkh10wz8t573m_c20xf00000gp/T/ipykernel_95737/3274215102.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hotel_cluster'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/SoftwareRepositories/spark-3.3.1-bin-hadoop3/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[0;32m~/SoftwareRepositories/spark-3.3.1-bin-hadoop3/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SoftwareRepositories/spark-3.3.1-bin-hadoop3/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SoftwareRepositories/spark-3.3.1-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Classifier inferred 1198786 from label values in column RandomForestClassifier_9a220eeb559c__labelCol, but this exceeded the max numClasses (100) allowed to be inferred from values.  To avoid this error for labels with > 100 classes, specify numClasses explicitly in the metadata; this can be done by applying StringIndexer to the label column."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 11:02:09 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 368429 ms exceeds timeout 120000 ms\n",
      "23/01/19 11:02:09 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol='hotel_cluster', featuresCol='features')\n",
    "rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c6df52-5690-4208-a59e-76a60581f1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
