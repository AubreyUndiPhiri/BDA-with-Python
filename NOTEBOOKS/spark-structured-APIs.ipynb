{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b33aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8374fe",
   "metadata": {},
   "source": [
    "# Introducing the DataFrames API\n",
    "In Spark, a DataFrame object consists of [Row](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Row.html) objects and [Column](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.html) objects. Concretely, each row of a Spark DataFrame  is an instance of the ```pyspark.sql.Row``` while each column is an instance of the ```pyspark.sql.Column``` class. We will look at  each of these classes in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3260a945",
   "metadata": {},
   "source": [
    "## Creating DataFrames\n",
    "1. From Python objects\n",
    "2. External data sources\n",
    "3. Other Spark objects\n",
    "\n",
    "### Schemas\n",
    "Also, when creating DataFrames, you have the option to use a schema or not. A schema in Spark defines the column names and associated data types for a DataFrame. Most often, schemas come into play when you are reading structured data from an external data source. When a schema is not used, Spark has to infer the data type which can slow your application if you have a massive  dataset. Although schemas are more of DBMS language but they offer several advantages when dealing with large datasets:\n",
    "- Spark doesnt have to infer data types, so you get speed benefits.\n",
    "- Without a schema, Spark creates a separate job just to read a large portion of your file to ascertain the schema, which for a large data file can be expensive and time-consuming. As such, defining a schema will avoid this.\n",
    "- You can detect errors early if data doesnâ€™t match the schema.\n",
    "#### Defining Schemas\n",
    "- Programmatically using Spark DataTypes \n",
    "- Using Data Definition Language (DDLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6d225",
   "metadata": {},
   "source": [
    "### Spark DataFrame from Python objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema using Spark DataTypes\n",
    "schema = StructType([StructField(\"author_name\", StringType(), False),\n",
    "      StructField(\"book_title\", StringType(), False),\n",
    "      StructField(\"num_pages\", IntegerType(), False)])\n",
    "\n",
    "# Define Schema using DDL\n",
    "schema = \"author_name STRING, book_title STRING, num_pages INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for our data using DDL\n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING,`Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe2bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple statistic data\n",
    "# in real life, we can get alot data in the o=form of Python objects and want to create SparkDataFrames\n",
    "# for instance, data being downloaded from websites\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n",
    "           [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
    "    \"LinkedIn\"]],\n",
    "           [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
    "    \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "           [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,\n",
    "    [\"twitter\", \"FB\"]],\n",
    "           [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
    "    \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "           [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,\n",
    "    [\"twitter\", \"LinkedIn\"]]\n",
    "          ]\n",
    "\n",
    "# Create a SparkSession\n",
    "# spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrameFromPythonObj\").getOrCreate()\n",
    "spark=SparkSession.builder.appName(\"intro\").master(\"local[*]\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame using the schema defined above\n",
    "sdf = spark.createDataFrame(data, schema)\n",
    "# Show the DataFrame; it should reflect our table above blogs_df.show()\n",
    "# Print the schema used by Spark to process the DataFrame\n",
    "print(sdf.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f917c",
   "metadata": {},
   "source": [
    "### EXERCISE-1: READ CSV WITH SCHEMA\n",
    "1. Use Spark documentation on how to read from file with a define schema. \n",
    "Note, the schema is what we arleady defined above. The data above has been saved as ```blog_simple_dataset.csv```. Read it as a Spark DataFrame with schema. Answer this question in the next cell.\n",
    "2. Define schema for the ```activity_raw_data.csv``` use string for the datetime column\n",
    "3. Load the dataset with and without schema using the functions defined below. Compare the loading times. Answer this question by completing the functions defined below and calling them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835604c5",
   "metadata": {},
   "source": [
    "### Spark DataFrame from external data sources\n",
    "The most common way (which we have already seen) is to load data from exteernal data sources and \n",
    "Spark supports numerous data stores. Spark reads data  through the ```DataFrameReaderobject```. Please look at the documeentation [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html) to see all data sources that the Spark  ```DataFrameReaderobject``` supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a23d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"SparkConnectors.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28273efc",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def timefn(fn):\n",
    "    \"\"\"\n",
    "    Function for recording running time of a function\n",
    "    \"\"\"\n",
    "    @wraps(fn)\n",
    "    def measure_time(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = fn(*args, **kwargs)\n",
    "        t2 = time.time()\n",
    "        print(\"@timefn:\" + fn.__name__ + \" took \" + str(t2 - t1) + \" seconds\")\n",
    "        return result\n",
    "    return measure_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290808a9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "@timefn\n",
    "def load_with_schema(large_csv):\n",
    "    # define the schema here\n",
    "    # you can load part of the file with pandas (just a few rows)\n",
    "    # to remind yourself of the data types\n",
    "    schema = YOUR CODE HERE \n",
    "    spark = SparkSession.builder.master(\"local[*]\").appName(\"ReadWithChema\").getOrCreate()\n",
    "    # Now read the data \n",
    "    sdf = YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59b7d6",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "@timefn\n",
    "def load_without_schema(large_csv):\n",
    "    spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrameFromPythonObj\").getOrCreate()\n",
    "    sdf = spark.read.csv(large_csv, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1dddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_with_schema(\"/Users/dmatekenya/Desktop/TMP/data/activity_log_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f177346",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_without_schema(\"/Users/dmatekenya/Desktop/TMP/data/activity_log_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478bbe8",
   "metadata": {},
   "source": [
    "## Columns and Expressions in  DataFrames\n",
    "In Spark DataFrames, columns behave like pandas DataFrames in several ways but they also behave different. You can list all the columns by their names, and you can perform operations on their values using relational or computational expressions. \n",
    "- [Column](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.html) is the name of the object, which has many import methods such as describe  while [col()](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.col.html) is a standard built-in function that returns a Column.\n",
    "\n",
    "We need to use the col() and expr() function available in pyspark,sql.functions() for many operations such as:\n",
    "- Add, rename columns\n",
    "- Subset data based on columns\n",
    "- Access columns to compute stats on them\n",
    "-  Access columns to compute operations on them such as sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33ffa3e",
   "metadata": {},
   "source": [
    "### Add a new column using expr and col\n",
    "In order to add a new column in a Spark DataFrame, we use the ```DataFrame.withColumn(new_col_name, expression_to_compute_new_col)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe979e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_fpath = \"/Users/dmatekenya/Desktop/TMP/data/activity_log_raw.csv\"\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrameFromPythonObj\").getOrCreate()\n",
    "sdf = spark.read.csv(csv_fpath, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9578195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use expr\n",
    "sdf2 = sdf.withColumn(\"new_col\", (expr(\"ACTIVITY_ID > 10000\")))\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c4723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the col function which I prefer over the expr col(\"Hits\")\n",
    "sdf2 = sdf.withColumn(\"new_col\", col(\"ACTIVITY_ID\") > 10000)\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32da731c",
   "metadata": {},
   "source": [
    "### Subset data  based on a few columns\n",
    "In order to access a single or multiple columns, we use the ```select()``` function on the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3 = sdf.select('ACTIVITY_TIME', 'STATUS')\n",
    "sdf3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5207966",
   "metadata": {},
   "source": [
    "**EXERCISE-2:**\n",
    "\n",
    "1. Check  if these statements: df.select(expr(\"ACTIVITY_TIME\")).show(2), df.select(col(\"ACTIVITY_TIME\")).show(2)\n",
    "and df.select(\"ACTIVITY_TIME\").show(2) will provide  the same output. Replace df with name of your Spark DataFrame.\n",
    "\n",
    "2. Create a new DataFrame using expr to get only those rows where STATUS is \"S\"\n",
    "Note that expr() just perfoms the operation, it doesnt filter our the rows which evaluate to false.\n",
    "2. Sort DataFrame: use the col function to sort the DataFrame on \"SID\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f3914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c9d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.sort(col(\"SID\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacb2f20",
   "metadata": {},
   "source": [
    "### Rows\n",
    "A row in Spark is a generic Row object, containing one or more columns. Each column may be of the same data type (e.g., integer or string), or they can have different types (integer, string, map, array, etc.). Because Row is an object in Spark and an ordered collection of fields, you can instantiate a Row the same way we instantiate any object. Consequently, you can collect Row objects in a list and create a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f0667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import Row\n",
    "row = Row(name=\"Alice\", age=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf4e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [Row(name=\"Matei Zaharia\", state=\"CA\"), Row(name=\"Reynold Xin\", state=\"CA\")]\n",
    "spark_df_from_rows = spark.createDataFrame(rows)\n",
    "spark_df_from_rows.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cdf3ac",
   "metadata": {},
   "source": [
    "**EXERCISE-3:** Creating a Spark DataFrame with Rows. Please complete the function below and call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f02fcd",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def convert_json_to_spark_with_rows(json_file):\n",
    "    # create  a list to hold all Row objects\n",
    "    rows = YOUR CODE\n",
    "    for idx, row in df.iterrows():\n",
    "        # get lon and lat from the coord column using indexing, dict key access\n",
    "        x = row['coord']['lon']       \n",
    "        y = row['coord']['lat']\n",
    "        # create the Row object here \n",
    "        srow = YOUR CODE\n",
    "        \n",
    "        # append this row object to the list\n",
    "        YOUR CODE\n",
    "    \n",
    "    # When creating Spark DataFrame this way, its better to use schema to avoid troubles\n",
    "    # create a schema for this data here, use DOUBLE as data type for lon and lat\n",
    "    schema = YOUR CODE\n",
    "    \n",
    "    # use spark.createDataFrame() here\n",
    "    # if yiu get errors, use the option verifySchema=False\n",
    "    spark_df = YOUR CODE\n",
    "    \n",
    "    # use show() statement to show the DataFrame\n",
    "    # use show() with print to ensure we see the outputs\n",
    "    YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce85f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfile = \"../data/city.list.json\"\n",
    "convert_json_to_spark_with_rows(jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee6d0f",
   "metadata": {},
   "source": [
    "# Common DataFrames Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5531c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE CONTINUED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e079ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_fpath = \"/Users/dmatekenya/Desktop/TMP/data/activity_log_raw.csv\"\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrameFromPythonObj\").getOrCreate()\n",
    "sdf = spark.read.csv(csv_fpath, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e115120",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf2 = sdf.select('ACTIVITY_TIME', 'STATUS')\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a28ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3 = sdf.select('ACTIVITY_TIME', 'STATUS')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
