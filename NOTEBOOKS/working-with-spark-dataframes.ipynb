{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f9639c-9ec8-436d-9d5b-951ec7264aa3",
   "metadata": {},
   "source": [
    "# Working with Spark DataFrames\n",
    "When it comes to all the strucutured APIs in Spark, the DataFrames API is probably the most commonly used way to \n",
    "interact with Spark functionality because its most familiar with most users. In this notebook, we will dive deeper into the DataFrames \n",
    "API and explore the following topics:\n",
    "- Creating Spark DataFrames with Schema and the advantage of using schemas\n",
    "- Manipulating DataFrames (e.g., adding colummns, renaming columns, changing data types etc\n",
    "- Query DataFrames. How do you filter data?\n",
    "- Performing common data analysis tasks on DataFrames (e.g., grouping data, applying functions)\n",
    "- Applying user defined functions in Spark DataaFrames\n",
    "- Switching between DataFrames and other data structures (e.g., Pandas, RDD's etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29611969",
   "metadata": {},
   "source": [
    "## Python setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b33aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "import time\n",
    "from IPython.display import Image\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bfdd8f-707e-4fa0-aa9e-0d53842475c4",
   "metadata": {},
   "source": [
    "## Inputs setup\n",
    "Lets provide paths to input files we will use. \n",
    "Its a good practice to create these as global variables. Also, use Python module ```Path``` from pathlib to manage file paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf12fd6-e5c0-48a8-962d-bc7d70eec5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altenatively, you can put a full-path to wheree your data is located like below\n",
    "# DATA_DIR = Path(full-path-folder-where-you-are-keeping-data)\n",
    "DATA_DIR = Path().cwd().parents[0].joinpath(\"DATA\")\n",
    "\n",
    "\n",
    "# Activity_log_raw\n",
    "ACTIV_LOG = DATA_DIR.joinpath(\"raw\", \"activity_log_raw.csv\")\n",
    "\n",
    "# path to hh_data.txt\n",
    "HH_DATA = DATA_DIR.joinpath(\"raw\", \"hh_data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8374fe",
   "metadata": {},
   "source": [
    "## Composition of a Spark DataFrame\n",
    "In Spark, a DataFrame object consists of [Row](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Row.html) objects and [Column](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.html) objects. Concretely, each row of a Spark DataFrame  is an instance of the ```pyspark.sql.Row``` while each column is an instance of the ```pyspark.sql.Column``` class. We will look at  each of these classes in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3260a945",
   "metadata": {},
   "source": [
    "## Creating DataFrames\n",
    "There are three main ways to create a Spark DataFrame as follows:\n",
    "1. From Python objects\n",
    "2. External data sources\n",
    "3. Other Spark objects\n",
    "\n",
    "### Schemas\n",
    "Also, when creating DataFrames, you have the option to use a schema or not. A schema in Spark defines the column names and associated data types for a DataFrame. Most often, schemas come into play when you are reading structured data from an external data source. When a schema is not used, Spark has to infer the data type which can slow your application if you have a massive  dataset. Although schemas are more of DBMS language but they offer several advantages when dealing with large datasets:\n",
    "- Spark doesnt have to infer data types, so you get speed benefits.\n",
    "- Without a schema, Spark creates a separate job just to read a large portion of your file to ascertain the schema, which for a large data file can be expensive and time-consuming. As such, defining a schema will avoid this.\n",
    "- You can detect errors early if data doesnâ€™t match the schema.\n",
    "#### Defining Schemas\n",
    "- Programmatically using Spark DataTypes \n",
    "- Using Data Definition Language (DDLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6d225",
   "metadata": {},
   "source": [
    "### Spark DataFrame from Python objects\n",
    "Given a list of data (in most cases its going to be a nested list), how do we convert it into a Spark DataFrame. There are several depending on the input Pytho objects. Here are some:\n",
    "1. Define a schema for the new dataframe. The schema helps Spark understand the column data types\n",
    "2. Convert the regular Python objects into ```Rows``` which we can the use to create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a4f458-da3a-4928-8145-3762b0a3b3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 14:09:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/14 14:09:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/02/14 14:09:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/02/14 14:09:53 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/02/14 14:09:53 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "# initialize SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrameFromPythonObj\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data \n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n",
    "           [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
    "    \"LinkedIn\"]],\n",
    "           [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
    "    \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "           [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,\n",
    "    [\"twitter\", \"FB\"]],\n",
    "           [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
    "    \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "           [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,\n",
    "    [\"twitter\", \"LinkedIn\"]]\n",
    "          ]\n",
    "# examine the data \n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for our data using DDL, in most cases whe \n",
    "schema_twitter = \"num INT, first_name STRING, last_name STRING, url STRING, date STRING, some_num INT, tweet STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b8207-2147-49ee-af0b-a96b836a4fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame using the schema defined above\n",
    "sdf_python_obj_schema = spark.createDataFrame(data, schema=schema_twitter)\n",
    "print(sdf_python_obj_schema.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44198c1e-b5a3-4ae2-9903-9259e6505af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of items into Spark Rows\n",
    "spark_rows = []\n",
    "\n",
    "for item in data:\n",
    "    row = Row(num=item[0], first_name=item[1], last_name=item[2], \n",
    "              url=item[3], date=item[4], some_num=item[5], tweet=item[6])\n",
    "    spark_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f2921-73b6-40f0-af58-ace81ecfdbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_from_rows = spark.createDataFrame(spark_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1975fb-f883-4be7-980a-30547cb0ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_from_rows.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dc7b48-766f-4d99-b172-32b919c88192",
   "metadata": {},
   "source": [
    "### Rows\n",
    "A row in Spark is a generic Row object, containing one or more columns. Each column may be of the same data type (e.g., integer or string), or they can have different types (integer, string, map, array, etc.). \n",
    "Because Row is an object in Spark and an ordered collection of fields, you can instantiate a Row the same way we instantiate any object. Consequently, you can collect Row objects in a list and create a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61a7938-f71f-414b-aff3-65f6aa884ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import row class\n",
    "from pyspark import Row\n",
    "\n",
    "first_five_rows = sdf.head(5)\n",
    "type(first_five_rows[0])\n",
    "\n",
    "# create row \n",
    "row = Row(name=\"Alice\", age=11)\n",
    "\n",
    "# A list of rows \n",
    "rows = [Row(name=\"Matei Zaharia\", state=\"CA\"), Row(name=\"Reynold Xin\", state=\"CA\")]\n",
    "\n",
    "# Create a DataFrame from Row objects\n",
    "spark_df_from_rows = spark.createDataFrame(rows)\n",
    "spark_df_from_rows.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6c768-cbe5-4f23-9d99-63feb037ce14",
   "metadata": {},
   "source": [
    "**EXERCISE-1:** Creating a Spark DataFrame with Rows. Please complete the function below and call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fc4e5-18a6-410b-b1d0-962bb1bbbf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_spark_with_rows(json_file):\n",
    "    # Load json into pandas DataFrame\n",
    "    \n",
    "    # create  a list to hold all Row objects\n",
    "    rows = YOUR CODE\n",
    "    for idx, row in df.iterrows():\n",
    "        # get lon and lat from the coord column using indexing, dict key access\n",
    "        x = row['coord']['lon']       \n",
    "        y = row['coord']['lat']\n",
    "        # create the Row object here \n",
    "        srow = YOUR CODE\n",
    "        \n",
    "        # append this row object to the list\n",
    "        YOUR CODE\n",
    "    \n",
    "    # When creating Spark DataFrame this way, its better to use schema to avoid troubles\n",
    "    # create a schema for this data here, use DOUBLE as data type for lon and lat\n",
    "    schema = YOUR CODE\n",
    "    \n",
    "    # use spark.createDataFrame() here\n",
    "    # if yiu get errors, use the option verifySchema=False\n",
    "    spark_df = YOUR CODE\n",
    "    \n",
    "    # use show() statement to show the DataFrame\n",
    "    # use show() with print to ensure we see the outputs\n",
    "    YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f195d9-36ce-403e-af54-449454d833cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfile = str(DATA_DIR.joinpath(\"raw\", \"city.list.json\")\n",
    "#convert_json_to_spark_with_rows(jsonfile)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bee41f",
   "metadata": {},
   "source": [
    "**EXERCISE-2: READ CSV WITH SCHEMA**\n",
    "1. Use Spark documentation on how to read from file with a define schema. \n",
    "Note, the schema is what we arleady defined above. The data above has been saved as ```blog_simple_dataset.csv```. Read it as a Spark DataFrame with schema. Answer this question in the next cell.\n",
    "2. Define schema for the ```activity_raw_data.csv``` use string for the datetime column\n",
    "3. Load the dataset with and without schema using the functions defined below. Compare the loading times. Answer this question by completing the functions defined below and calling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7556d4e2",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def timefn(fn):\n",
    "    \"\"\"\n",
    "    Function for recording running time of a function\n",
    "    \"\"\"\n",
    "    @wraps(fn)\n",
    "    def measure_time(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = fn(*args, **kwargs)\n",
    "        t2 = time.time()\n",
    "        print(\"@timefn:\" + fn.__name__ + \" took \" + str(t2 - t1) + \" seconds\")\n",
    "        return result\n",
    "    return measure_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c823db",
   "metadata": {
    "code_folding": [
     1
    ],
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@timefn\n",
    "def load_with_schema(large_csv):\n",
    "    # define the schema here using DDL\n",
    "    # you can load part of the file with pandas (just a few rows)\n",
    "    # to remind yourself of the data types\n",
    "    schema = \"`SID` INT, `ACTIVITY_ID` INT, `Last` STRING, `ACTIVITY_TIME` STRING,`STATUS` STRING\"\n",
    "    spark = SparkSession.builder.master(\"local[*]\").appName(\"ReadWithChema\").getOrCreate()\n",
    "    # Now read the data \n",
    "    sdf = spark.read.schema(schema).csv(large_csv)\n",
    "    \n",
    "    print(sdf.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd849bb",
   "metadata": {
    "code_folding": [
     1
    ],
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@timefn\n",
    "def load_without_schema(large_csv):\n",
    "    spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrameFromPythonObj\").getOrCreate()\n",
    "    sdf = spark.read.csv(large_csv, header=True)\n",
    "    print(sdf.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2de6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_with_schema(str(ACTIV_LOG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22962c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_without_schema(str(ACTIV_LOG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101580b-c996-47da-aa8c-34ce8f03bdd5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "The difference shown above will of course become signficant and consequential when the number of columns and size of data increase. As such =, although not required, in some use cases, you can use schema to improve performance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82841192",
   "metadata": {},
   "source": [
    "### Spark DataFrame from external data sources\n",
    "The most common way (which we have already seen) is to load data from exteernal data sources and \n",
    "Spark supports numerous data stores. Spark reads data  through the ```DataFrameReaderobject```. Please look at the documeentation [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html) to see all data sources that the Spark  ```DataFrameReaderobject``` supports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602cc1ac-7696-4808-9aa7-d3ad16ff7348",
   "metadata": {},
   "source": [
    "## Manipulating Spark DataFrames \n",
    "\n",
    "### Columns and Expressions in  DataFrames\n",
    "In Spark DataFrames, columns behave like pandas DataFrames in several ways but they also behave different. You can list all the columns by their names, and you can perform operations on their values using relational or computational expressions. \n",
    "- [Column](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.html) is the name of the object, which has many import methods such as describe  while [col()](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.col.html) is a standard built-in function that returns a Column.\n",
    "\n",
    "We need to use the col() and expr() function available in pyspark,sql.functions() for many operations such as:\n",
    "- Add, rename columns\n",
    "- Subset data based on columns\n",
    "- Access columns to compute stats on them\n",
    "- Access columns to compute operations on them such as sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a593ff-6085-4d78-a4c9-b533961617a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use Activity_log_raw.csv for trying out manipulations\n",
    "sdf = spark.read.csv(str(ACTIV_LOG), header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6922f251-b81e-4618-a6ab-6be4fd29dfb7",
   "metadata": {},
   "source": [
    "### Selecting columns and rows \n",
    "How to select a single column and/or a subset of rows. We use ```select()``` to pick one or multipe. \n",
    "However, some of the same indexing we use in Pandas work, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eba23d-60fd-44dc-9c7b-e7bcf8f471ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unnique values in column 'P21'\n",
    "# Note that you have to use colleect() or show() \n",
    "# to bring results to driver and see, other this is a transformation\n",
    "sdf2 = sdf.select('ACTIVITY_TIME', 'STATUS')\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90a31a-9bfa-473e-8fba-0bb01a67c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use of the Pandas like indexing seem to also be supported like this \n",
    "# but just experiment with it because not everything is supported \n",
    "sdf_two_cols = sdf[['ACTIVITY_TIME', 'STATUS']]\n",
    "sdf_two_cols.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a33802",
   "metadata": {},
   "source": [
    "### Add a new column using expr and col\n",
    "In order to add a new column in a Spark DataFrame, we use the ```DataFrame.withColumn(new_col_name, expression_to_compute_new_col)```. \n",
    "For that, we need to import the ```col``` and ```expr``` functions from the ```pyspark.sql.functions``` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e911533-df3f-47a1-964e-353c10c83f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use expr\n",
    "sdf2 = sdf.withColumn(\"status_S\", (expr('STATUS = \"S\"')))\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147bcc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the col function which I prefer over the expr col(\"Hits\")\n",
    "sdf2 = sdf.withColumn(\"new_col\", col(\"STATUS\") == 'S')\n",
    "sdf2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1439c",
   "metadata": {},
   "source": [
    "**EXERCISE-3:**\n",
    "\n",
    "1. Check  if these statements: df.select(expr(\"ACTIVITY_TIME\")).show(2), df.select(col(\"ACTIVITY_TIME\")).show(2)\n",
    "and df.select(\"ACTIVITY_TIME\").show(2) will provide  the same output. Replace df with name of your Spark DataFrame.\n",
    "\n",
    "2. Create a new DataFrame using expr to get only those rows where STATUS is \"S\"\n",
    "Note that expr() just perfoms the operation, it doesnt filter our the rows which evaluate to false.\n",
    "2. Sort DataFrame: use the col function to sort the DataFrame on \"SID\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c62d9f9-12e2-4eab-a2fa-d952fddd39f4",
   "metadata": {},
   "source": [
    "**EXERCISE-4: USING THE HH_DATA, TRY OUT THESE COMMON OPERATIONS ON A DATAFRAME**\n",
    "1. Using the ```filter()``` function create a DataFrame of people who are 5 years and younger?\n",
    "2. Find **household size** using ```groupby()``` on the hh_id column and appropriate function (e.g., count, avg)\n",
    "3. Add the household size to the ```sdf_hh``` dataframe\n",
    "4. Rename columns as follows: ```P08-->age```, ```P07M-->dob_month``` and ```P07A-->dob_yr```\n",
    "5. Find how many null values are there in the age column and drop all nulls in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788f0d45-6390-4e6d-8ef6-11989f808103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the dataframe\n",
    "sdf_hh = spark.read.csv(str(HH_DATA),header=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9bfa11-de32-421c-b28e-b8521d13d722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 14:10:25 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , urban_rural, hh_id, P03, P05, P07M, P07A, P08, P21, P22N, P23, P28, P29\n",
      " Schema: _c0, urban_rural, hh_id, P03, P05, P07M, P07A, P08, P21, P22N, P23, P28, P29\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/dmatekenya/TEACHING-LOCAL/BDA-with-Python/DATA/raw/hh_data.txt\n",
      "+---+-----------+--------------------+---+---+----+----+---+---+----+---+---+---+\n",
      "|_c0|urban_rural|               hh_id|P03|P05|P07M|P07A|P08|P21|P22N|P23|P28|P29|\n",
      "+---+-----------+--------------------+---+---+----+----+---+---+----+---+---+---+\n",
      "|  0|          1|11101101010011066...|  0|  1|  10|1954| 63|  1|   3|  6|  2| 24|\n",
      "|  1|          1|11101101010011066...|  1|  2|   8|1950| 67|  1|   3|  6|  2| 28|\n",
      "|  2|          1|11101101010011066...|  2|  1|   3|1980| 38|  1|   5|  1|  1|   |\n",
      "|  3|          1|11101101010011066...|  2|  1|   9|1984| 33|  1|   5|  2|  1|   |\n",
      "|  4|          1|11101101010011066...|  0|  1|  12|1984| 33|  1|   5|  1|  2| 25|\n",
      "+---+-----------+--------------------+---+---+----+----+---+---+----+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_hh.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c15dac95-405f-4bf9-8221-727217769108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==============>                                          (3 + 9) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4,448,568 under-five children in the country out of a total of 25,674,196 people\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Question-1\n",
    "# Get dataframe for undr 5 years\n",
    "sdf_hh2 = sdf_hh.withColumnRenamed('P08', 'age')\n",
    "sdf_hh_u5 = sdf_hh2.filter('age <= 5')\n",
    "total_pop = sdf_hh.count()\n",
    "cnt_u5_children = sdf_hh_u5.count()\n",
    "print('There are {:,} under-five children in the country out of a total of {:,} people'.format(\n",
    "    cnt_u5_children, total_pop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d46d53d-03fb-4982-9adf-25fe8b037b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               hh_id|count|\n",
      "+--------------------+-----+\n",
      "|11101101010021060...|    7|\n",
      "|11101101010031065...|    1|\n",
      "|11101101010031065...|    6|\n",
      "|11101101010031066...|    7|\n",
      "|11101101020011036...|    3|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Question-2\n",
    "# Create a new dataframe called sdf_hh_size which \n",
    "# gives number of persons per household by grouping by \n",
    "# hh_id\n",
    "sdf_hh_size = sdf_hh2.groupby('hh_id').count()\n",
    "sdf_hh_size.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d588d7b-c4be-4aeb-9321-388869c8f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the hh_size column to the sdf_hh dataframe\n",
    "# using join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6168578-cad1-4d7d-b390-51034f736bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>               (0 + 12) / 12][Stage 35:>                (0 + 0) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 14:37:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , urban_rural, hh_id, P03, P05, P07M, P07A, P08, P21, P22N, P23, P28, P29\n",
      " Schema: _c0, urban_rural, hh_id, P03, P05, P07M, P07A, P08, P21, P22N, P23, P28, P29\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/dmatekenya/TEACHING-LOCAL/BDA-with-Python/DATA/raw/hh_data.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------+---+---+----+----+---+---+----+---+---+---+-----+\n",
      "|               hh_id| _c0|urban_rural|P03|P05|P07M|P07A|age|P21|P22N|P23|P28|P29|count|\n",
      "+--------------------+----+-----------+---+---+----+----+---+---+----+---+---+---+-----+\n",
      "|11101101010021060...|1719|          1|  0|  1|   5|1976| 42|  1|   5|  1|  2| 34|    7|\n",
      "|11101101010021060...|1720|          1|  1|  2|  10|1985| 32|  1|   5|  1|  2| 25|    7|\n",
      "|11101101010021060...|1721|          1|  2|  1|  10|2010|  7|  2|   2|  5|   |   |    7|\n",
      "|11101101010021060...|1722|          1|  2|  1|   7|2013|  4|  2|   1|   |   |   |    7|\n",
      "|11101101010021060...|1723|          1|  2|  1|   4|2016|  2|   |    |   |   |   |    7|\n",
      "+--------------------+----+-----------+---+---+----+----+---+---+----+---+---+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf_hh3 = sdf_hh2.join(sdf_hh_size, on='hh_id')\n",
    "sdf_hh3.cache()\n",
    "sdf_hh3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2293fdf-24a7-40c2-92cb-05dedc34903c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 14:35:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , urban_rural, hh_id, P03, P05, P07M, P07A, P08, P21, P22N, P23, P28, P29\n",
      " Schema: _c0, urban_rural, hh_id, P03, P05, P07M, P07A, P08, P21, P22N, P23, P28, P29\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/dmatekenya/TEACHING-LOCAL/BDA-with-Python/DATA/raw/hh_data.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----------+---+---+----+----+---+---+----+---+---+---+-------+\n",
      "|               hh_id|_c0|urban_rural|P03|P05|P07M|P07A|age|P21|P22N|P23|P28|P29|hh_size|\n",
      "+--------------------+---+-----------+---+---+----+----+---+---+----+---+---+---+-------+\n",
      "|11101101010011066...| 48|          1|  0|  1|   2|1988| 30|  1|   3|  1|  2| 24|      4|\n",
      "|11101101010011066...| 49|          1|  1|  2|   5|1991| 27|  1|   3|  4|  2| 21|      4|\n",
      "|11101101010011066...| 50|          1|  2|  2|   2|2013|  5|  2|   1|  5|   |   |      4|\n",
      "|11101101010011066...| 51|          1|  3|  2|   1|1948| 70|  1|   3|  6|  4| 22|      4|\n",
      "|11101101010011066...|191|          1|  0|  2|   2|1961| 57|  1|   3|  1|  2| 26|      5|\n",
      "+--------------------+---+-----------+---+---+----+----+---+---+----+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# change column name from 'count' to hh_size\n",
    "sdf_hh4 = sdf_hh3.withColumnRenamed('count', 'hh_size')\n",
    "sdf_hh4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34fff4e6-fdaf-49e9-8356-cd0f09a136ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------+---+---+---------+------+---+---+----+---+---+---+-------+\n",
      "|               hh_id| _c0|urban_rural|P03|P05|dob_month|dob_yr|age|P21|P22N|P23|P28|P29|hh_size|\n",
      "+--------------------+----+-----------+---+---+---------+------+---+---+----+---+---+---+-------+\n",
      "|11101101010021060...|1719|          1|  0|  1|        5|  1976| 42|  1|   5|  1|  2| 34|      7|\n",
      "|11101101010021060...|1720|          1|  1|  2|       10|  1985| 32|  1|   5|  1|  2| 25|      7|\n",
      "|11101101010021060...|1721|          1|  2|  1|       10|  2010|  7|  2|   2|  5|   |   |      7|\n",
      "|11101101010021060...|1722|          1|  2|  1|        7|  2013|  4|  2|   1|   |   |   |      7|\n",
      "|11101101010021060...|1723|          1|  2|  1|        4|  2016|  2|   |    |   |   |   |      7|\n",
      "+--------------------+----+-----------+---+---+---------+------+---+---+----+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question-: Rename columns as follows: \n",
    "# P08-->age, P07M-->dob_month and P07A-->dob_yr\n",
    "sdf_hh5 = (sdf_hh4\n",
    "           .withColumnRenamed('P07M',  'dob_month')\n",
    "           .withColumnRenamed('P07A',  'dob_yr'))\n",
    "sdf_hh5.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571398d5-4b5a-4e8b-84fb-735aa55ccce7",
   "metadata": {},
   "source": [
    "## Spark-SQL functions\n",
    "As usual refer to the Pyspark API documentation for an overview of spark modules and core classes. So far, we have been \n",
    "interacting with spark-SQL. Within the spark-SQL, we have worked with the following:\n",
    "- DataFrame API\n",
    "- Row\n",
    "- Spark Session API\n",
    "- Functions\n",
    "\n",
    "For functions, we have only used simple functions such as ```count``` and ```avg``` used in aggregation. Now, lets look at user defined functions (udf). Although new and more functions are continuously being added to the spark APIs, you will often find yourself in a situation where you need to define a small Python function to apply on spark DataFrame or other data types in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8afc2e0-cc5d-4238-a70d-d39057aa7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"../DOCS/images/spark-functions.png\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81700c-6fa1-46be-896b-ecb9c7fcb762",
   "metadata": {},
   "source": [
    "### Creating and using user defined functions\n",
    "Spark provides two types of user defined functions (UDF):\n",
    "1. [Simple UDF](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html#pyspark.sql.functions.udf).\n",
    "We use the ```udf``` keyword to define a UDF using lambda. When using the ```def``` keyword, we use ```udf``` as a decorator. When defining UDF, specifying the Spark Data type to be returned is important though in some cases (e.g., for strings) you can getaway without specifying the data type. Once defined, you can call the UDF by using ```col()``` to select a single column or multiple columns as input into the UDF.\n",
    "2. [Pandas UDF](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf). With pandas on Spark API now availablee, not sure this function is useful anymore but it probably is. Esseentially, you use pandas functitons on a small slice of a Spark DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a48ed5-d540-4d14-a20a-c79cb503408d",
   "metadata": {},
   "source": [
    "# How can we use user defined functions in this dataframe?\n",
    "For this ```sdf``` dataframe, I want to know the earliest activity date. For that, I need to convert ```ACTIVITY_TIME``` from String to Spark ```DateType``` or ```TimestampType```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc9372c1-76e0-497c-9554-7b6fb4b2b3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+------+\n",
      "|SID|ACTIVITY_ID|       ACTIVITY_TIME|STATUS|\n",
      "+---+-----------+--------------------+------+\n",
      "|584|       1291|13-APR-15 10.33.4...|     S|\n",
      "|584|       1286|13-APR-15 10.33.4...|     S|\n",
      "|584|       1285|13-APR-15 10.33.4...|     S|\n",
      "|584|       1284|13-APR-15 10.33.4...|     S|\n",
      "|584|       1288|13-APR-15 10.33.4...|     S|\n",
      "+---+-----------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_act = spark.read.csv(str(ACTIV_LOG), header=True)\n",
    "sdf_act.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "794dc744-5818-4bb4-ad02-4b43c89a170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Spark UDF to add date and datetime with lambda setup\n",
    "# Note that in this case, the return data typee is Spark TimestampType(), DateType()\n",
    "# date_fmt1='%Y%m%d%H%M%S'\n",
    "\n",
    "# For dateitme, you need to provide the datetime format of the string\n",
    "# please see here: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "# for more details on converting from datetime string \n",
    "# to datetime Python object\n",
    "date_fmt2 = \"%d-%b-%y %H.%M.%S\"\n",
    "\n",
    "# define UDF\n",
    "add_datetime = udf(lambda x: datetime.strptime(x[:-13], date_fmt2), TimestampType())\n",
    "add_date = udf(lambda x: datetime.strptime(x[:-13], date_fmt2), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb0f814c-29cd-41a3-ab4d-eaf8492cbdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+------+-------------------+----------+\n",
      "|SID|ACTIVITY_ID|       ACTIVITY_TIME|STATUS|           datetime|      date|\n",
      "+---+-----------+--------------------+------+-------------------+----------+\n",
      "|584|       1291|13-APR-15 10.33.4...|     S|2015-04-13 10:33:42|2015-04-13|\n",
      "|584|       1286|13-APR-15 10.33.4...|     S|2015-04-13 10:33:42|2015-04-13|\n",
      "|584|       1285|13-APR-15 10.33.4...|     S|2015-04-13 10:33:42|2015-04-13|\n",
      "|584|       1284|13-APR-15 10.33.4...|     S|2015-04-13 10:33:42|2015-04-13|\n",
      "|584|       1288|13-APR-15 10.33.4...|     S|2015-04-13 10:33:42|2015-04-13|\n",
      "+---+-----------+--------------------+------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add datetime and date column with our functions above\n",
    "sdf_act2 = sdf_act.withColumn('datetime', add_datetime(col('ACTIVITY_TIME')))\n",
    "sdf_act3 = sdf_act2.withColumn('date', add_date(col('ACTIVITY_TIME')))\n",
    "sdf_act3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df00623e-508e-4cd4-a1e8-3b99667f464b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.               (0 + 12) / 63]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dmatekenya/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/dmatekenya/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/dmatekenya/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w3/dfvgjkh10wz8t573m_c20xf00000gp/T/ipykernel_73734/2973014246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sort by date find the earliest and oldest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msdf_act3_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdf_act3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mearliest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdf_act3_sorted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/SoftwareRepositories/spark-3.3.1-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1922\u001b[0m         \"\"\"\n\u001b[1;32m   1923\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SoftwareRepositories/spark-3.3.1-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1924\u001b[0m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1926\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SoftwareRepositories/spark-3.3.1-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \"\"\"\n\u001b[0;32m--> 868\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SoftwareRepositories/spark-3.3.1-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \"\"\"\n\u001b[1;32m    816\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:====================>                                  (23 + 12) / 63]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/15 05:38:39 ERROR Executor: Exception in task 34.0 in stage 46.0 (TID 244)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/w3/dfvgjkh10wz8t573m_c20xf00000gp/T/ipykernel_73734/873190141.py\", line 12, in <lambda>\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:====================>                                  (23 + 11) / 63]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "23/02/15 05:38:39 WARN TaskSetManager: Lost task 34.0 in stage 46.0 (TID 244) (172.29.175.158 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/w3/dfvgjkh10wz8t573m_c20xf00000gp/T/ipykernel_73734/873190141.py\", line 12, in <lambda>\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "23/02/15 05:38:39 ERROR TaskSetManager: Task 34 in stage 46.0 failed 1 times; aborting job\n",
      "23/02/15 05:38:39 WARN TaskSetManager: Lost task 35.0 in stage 46.0 (TID 245) (172.29.175.158 executor driver): TaskKilled (Stage cancelled)\n",
      "23/02/15 05:38:39 WARN TaskSetManager: Lost task 30.0 in stage 46.0 (TID 240) (172.29.175.158 executor driver): TaskKilled (Stage cancelled)\n",
      "23/02/15 05:38:40 WARN TaskSetManager: Lost task 26.0 in stage 46.0 (TID 236) (172.29.175.158 executor driver): TaskKilled (Stage cancelled)\n",
      "23/02/15 05:38:40 WARN TaskSetManager: Lost task 27.0 in stage 46.0 (TID 237) (172.29.175.158 executor driver): TaskKilled (Stage cancelled)\n",
      "23/02/15 05:38:40 WARN TaskSetManager: Lost task 20.0 in stage 46.0 (TID 230) (172.29.175.158 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:====================>                                   (23 + 7) / 63]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/15 05:38:40 WARN TaskSetManager: Lost task 32.0 in stage 46.0 (TID 242) (172.29.175.158 executor driver): TaskKilled (Stage cancelled)\n",
      "23/02/15 05:38:40 WARN TaskSetManager: Lost task 33.0 in stage 46.0 (TID 243) (172.29.175.158 executor driver): TaskKilled (Stage cancelled)\n",
      "23/02/15 05:38:40 WARN TaskSetManager: Lost task 31.0 in stage 46.0 (TID 241) (172.29.175.158 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:====================>                                   (23 + 4) / 63]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/15 05:38:40 WARN TaskSetManager: Lost task 28.0 in stage 46.0 (TID 238) (172.29.175.158 executor driver): TaskKilled (Stage cancelled)\n",
      "23/02/15 05:38:40 WARN TaskSetManager: Lost task 25.0 in stage 46.0 (TID 235) (172.29.175.158 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:====================>                                   (23 + 3) / 63]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/15 05:38:40 WARN TaskSetManager: Lost task 24.0 in stage 46.0 (TID 234) (172.29.175.158 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:====================>                                   (23 + 1) / 63]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/15 05:38:41 WARN TaskSetManager: Lost task 29.0 in stage 46.0 (TID 239) (172.29.175.158 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "# Sort by date find the earliest and oldest\n",
    "sdf_act3_sorted = sdf_act3.sort(col('date'))\n",
    "earliest = sdf_act3_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e1de94-d373-4d9c-abb5-2f66d50b09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_act3.sort?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50c43c-dd38-4386-bea4-03a267b28344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Try outt UDF on the activity_log_raw file\n",
    "sdf = spark.read.csv(CSV_PATH, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac84cdf-f2da-4fe3-99e1-beafa7b115b0",
   "metadata": {},
   "source": [
    "**EXERCISE-5: AGE FROM DOB**\n",
    "\n",
    "Use the hh_data.txt to do the following:\n",
    "1. create a column called ```age_from_dob``` which calculates age from the dob_yr column.\n",
    "2. Drop all observations where ```age_from_dob``` and ```age``` dont match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d49a6-b555-49a7-a769-dce19fc87934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
