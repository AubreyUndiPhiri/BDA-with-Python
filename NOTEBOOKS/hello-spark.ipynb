{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36260f58",
   "metadata": {},
   "source": [
    "# Hello Spark\n",
    "In this tutorial, you will run our first Spark application in Python. Thus, this our Spark Driver process.\n",
    "We will cover the following items:\n",
    "- Build a SparkSession\n",
    "- Use the high level API DataFrame to read a CSV file\n",
    "- Compare running time between Spark and Pandas when reading this large CSV file\n",
    "- Explore basic Spark DataFrames functionality\n",
    "- Explore the Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d11eb1",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Import required Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde5a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "import time\n",
    "from functools import wraps\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7ed47",
   "metadata": {},
   "source": [
    "# Create a SparkSession\n",
    "As we mentioned during the lecture, there is one main way to interact \n",
    "with Spark and thats using the SparkSession object. Therefore, the very first when writing a \n",
    "Spark application is to instantiate a SparkSession object. A SparkSession can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. There\n",
    "can only be one SparkSession per JVM. When creating a SparkSession, there are several options/parameters you can pass. For instance, pass number of cores. We can use ```*``` to instruct Spark to use all cores available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba84f1-3fb9-4c7d-a5f7-782ea8576233",
   "metadata": {},
   "source": [
    "##  Initialzing Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3fa93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 05:57:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Hello Spark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2f3a2",
   "metadata": {},
   "source": [
    "# Read Data into a Spark DataFrame\n",
    "We will get into details about the SparkDataFrames API but for now, its enough for you \n",
    "to understand that we can use the SparkSession to read data from different sources such as CSV as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c288a66c-4865-47d8-8fa5-38c6eb73c39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.csv(\"../DATA/raw/simulated_cdrs/\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "183ca5fa-941a-4c78-a84d-faac34a0efb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_id</th>\n",
       "      <th>cell_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S231</td>\n",
       "      <td>12221</td>\n",
       "      <td>-8.66928</td>\n",
       "      <td>26.9279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S231</td>\n",
       "      <td>12222</td>\n",
       "      <td>-8.66928</td>\n",
       "      <td>26.9279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S231</td>\n",
       "      <td>12223</td>\n",
       "      <td>-8.66928</td>\n",
       "      <td>26.9279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S231</td>\n",
       "      <td>12227</td>\n",
       "      <td>-8.66928</td>\n",
       "      <td>26.9279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S231</td>\n",
       "      <td>12228</td>\n",
       "      <td>-8.66928</td>\n",
       "      <td>26.9279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  site_id  cell_id      lat      lon\n",
       "0    S231    12221 -8.66928  26.9279\n",
       "1    S231    12222 -8.66928  26.9279\n",
       "2    S231    12223 -8.66928  26.9279\n",
       "3    S231    12227 -8.66928  26.9279\n",
       "4    S231    12228 -8.66928  26.9279"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 08:56:39 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 778800 ms exceeds timeout 120000 ms\n",
      "23/02/17 08:56:39 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "sim_locs = pd.read_csv(\"/Users/dmatekenya/Downloads/simulated_locs.csv\")\n",
    "sim_locs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e58eb819-a571-4c32-8851-8a98ad6234fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cdr type', 'cdr datetime', 'call duration', 'last calling cellid', 'user_id']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de44226c-6cde-475a-a3c0-0cb4b051e41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cdr type='MtSMSRecord', cdr datetime='20180710084407', call duration=None, last calling cellid=None, user_id='7566424924061690786')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_locs =c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3981ebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# please use the kaggle_expedia_train.csv  file here\n",
    "large_csv = \"../DATA/raw/activity_log_raw.csv\"\n",
    "df = spark.read.csv(large_csv, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d02d8b-1716-4c32-86dd-ba0d4d9a3069",
   "metadata": {},
   "source": [
    "# Explore Spark UI\n",
    "Lets run something which takes long (e.g., ```summary```) and then check whats happening on ```localhost:4040/``` in your browser. \n",
    "If ```localhost:4040/``` doesnt work, you can try ```localhost:4041/```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff2b848-981c-41e1-9849-20728d167364",
   "metadata": {},
   "source": [
    "# Partitions\n",
    "You can check and channge number of partitions on the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af91b30-16a0-403b-8b15-3b6a0a1cced5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "num_partitions = df.rdd.getNumPartitions()\n",
    "\n",
    "print(num_partitions)\n",
    "\n",
    "df2 = df.repartition(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4cc11-68b2-4a53-9627-8950cf88c75c",
   "metadata": {},
   "source": [
    "# Transformations vs. Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea2d69-92a7-45c9-9895-220a0edba223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample() is a transformation and nothing happens you execute the cell\n",
    "pdf_sample = df.sample(fraction=0.1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a3ef2-17e6-4d50-a300-c2a820d62888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count() is an action because Spark has to actually count\n",
    "# So, Spark executes the sample first and then do count\n",
    "df_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b020fe-4e0f-45bf-bc2a-9aa0384fef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example actions: show(), head() are actions \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02331e25",
   "metadata": {},
   "source": [
    "# Compare running time for Spark and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71db01d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def timefn(fn):\n",
    "    \"\"\"\n",
    "    Function for recording running time of a function\n",
    "    \"\"\"\n",
    "    @wraps(fn)\n",
    "    def measure_time(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = fn(*args, **kwargs)\n",
    "        t2 = time.time()\n",
    "        print(\"@timefn:\" + fn.__name__ + \" took \" + str(t2 - t1) + \" seconds\")\n",
    "        return result\n",
    "    return measure_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750f6cb2",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "@timefn\n",
    "def load_big_csv_with_spark(big_csv=None, spark_session=None):\n",
    "    \"\"\"\n",
    "    A simple function which loads a CSV file using Apache Spark and\n",
    "    then counts how many rows are in the file\n",
    "    \"\"\"\n",
    "    # create a Spark Session here\n",
    "    # read the CSV  file int Spark DataFrame\n",
    "    df = spark_session.read.csv(big_csv, header=True)\n",
    "    # Get the number of rows in the dataset using the count() function\n",
    "    cnt = df.count()\n",
    "    print('Number of rows in big CSV file: {:,}'.format(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e7909",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "@timefn\n",
    "def load_big_csv_with_pandas(big_csv=None):\n",
    "    \"\"\"\n",
    "    Use pandas library to load the large CSV\n",
    "    \"\"\"\n",
    "    # Read CSV as a pandas DataFrame (df) here\n",
    "    df = pd.read_csv(big_csv)\n",
    "    \n",
    "    # Get the total number of rows\n",
    "    cnt = df.shape[0]\n",
    "   \n",
    "    print('Number of rows in big CSV file: {:,}'.format(cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c156946e-14cf-407d-89ed-ef9cdc572bfa",
   "metadata": {},
   "source": [
    "**EXERCISE-1**: Compare running time for spark and pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64becb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call the two functions above here\n",
    "#load_big_csv_with_pandas(big_csv=large_csv)\n",
    "load_big_csv_with_spark(big_csv=large_csv, spark_session=spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d196630d",
   "metadata": {},
   "source": [
    "# Exploring DataFrames in Spark\n",
    "We can do the similar things we did with pandas dataframes in order to explore the data.\n",
    "- We can use the `printSchema()` function to check column data types as well as what kind of columns we have.\n",
    "- Also, you can use the the same `columns` function to get a list of all columns\n",
    "- We can use `head()` function just like in pandas to get the top `n` observations from the data. Note that you can use n within the brackets in the head function to specify the number of rows you want to see.\n",
    "- Count the number of rows in the dataframe using the `count()` function\n",
    "- Get number of unique elements using `distinct()` command. If you want number of unique elements based on a single column. You can first select the column using the sysntax `df.select(\"column\").distinct().count()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a5173",
   "metadata": {},
   "source": [
    "**EXERCISE-2**: Exploring spark dataframe\n",
    "1. Use ```head()``` function to view the first 5 observations\n",
    "2. Check column data types\n",
    "3. How many unique categories are there for the STATUS variable?\n",
    "4. Using the documentation for [Pyspark DataFrame](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) find a function to get a sample of the data. Get a sample of 10% of the data and convert it to a pandas dataframe using `toPandas()` function\n",
    "\n",
    "```Although there is a pandas API for spark, we will not get into just yet, lets explore core functionality of spark first```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5812da",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Explore Spark UI\n",
    "In order to explore the Spark UI, we will run a function that take time to run. \n",
    "For this, lets run the ```summary()``` funciton. Once you call the function, open the spakr UI by going to this url: *localhost/4040*. Once there, explore the different tabs:\n",
    "- **Jobs:** On the Jobs tab, you can check the  status of your current job. You can see the exact functions such as count that  Spark is running\n",
    "- **Executors:** In this tab, you can check how cores is Spark using and how tasks are being run in each core\n",
    "- **Stages:**. You can at which stage the job is running, how many tasks have completed\n",
    "- **Environment:** Its important to see which environment variables Spark is using, you can check that using this tab.\n",
    "- **Storage, SQL:** Explore these tabs to see what information they contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c73942-b6a8-40d5-8037-40150cdfc19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(large_csv, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d160beff",
   "metadata": {},
   "source": [
    "If the summary job is taking too long, you can kill it using the ```kill``` function in Spark UI because in some cases, its impossible to stop it in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c77be-a87f-4a3d-8f9a-024c248ad985",
   "metadata": {},
   "source": [
    "# Other ways to run spark\n",
    "1. **spark-shell**: For quick and faster interaction with spark.\n",
    "2. **spark-submit**. In the terminal, often for submitting jobs in clusters but \n",
    "you can also use it in local mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa088a1-64c2-46d5-b6ed-f0a66b038923",
   "metadata": {},
   "source": [
    "**EXERCISE-3**: Run spark in shell\n",
    "1. Locate spark folder using step-1 below.\n",
    "2. Navigate to the ```bin``` directory\n",
    "3. Run the ```pyspark``` command in there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f4998",
   "metadata": {},
   "source": [
    "**EXERCISE-4**: Running spark using ```spark-submit```\n",
    "\n",
    "Running spark using ```spark-submit``` and comparing Spark Running time based on the number of executors assigned to Spark. Unfortunatel, in Jupyter notebook, setting the number of executors isnt working well and so we will have to do it in terminal to explore this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d1d7f",
   "metadata": {},
   "source": [
    "## Step-1: Identify location of your Spark installation. \n",
    "- Run code below to note down the base folder of you Spark\n",
    "- Make sure you identify the root of the Spark folder\n",
    "- On the terminal, navigate to that folder using cd and then navigate to the ```bin``` folder\n",
    "- If you run the ```ls``` command while in the ```bin``` folder, you should see a ```spark-submit``` executable\n",
    "- ```spark-submit``` is used to submit standalone Spark applications  to a cluster but we can use it in local mode too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7074e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code below to note down the base folder of you Spark\n",
    "# Make sure you identify the root of the Spark folder\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1966f-bc64-4250-b1d5-66090c636c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1baf90",
   "metadata": {},
   "source": [
    "## Step-2: Create a ```.py``` file in VS Code or any text editor\n",
    "- Copy all the necessary imports and add them at the top of the Python file\n",
    "- Next, add this line of code as its always required when running Python scripts: \n",
    "```if __name__ == \"__main__\":```\n",
    "- Copy the code from just after this heading: ```Compare running time for Spark and Pandas``` to before the heading: ```Exploring DataFrames in Spark```. And paste the code underneath the statement above.\n",
    "- Make sure your Python file has no errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a932e12",
   "metadata": {},
   "source": [
    "## Step-3: Run the Spark Application\n",
    "1. Note the full path of your Python file\n",
    "2. On the terminal, navigate to the Spark ```cd```\n",
    "3. Within the Spark folder, run this command:\n",
    "```./bin/spark-submit --name \"Hello Spark\" --master local[num_executors] --conf spark.eventLog.enabled=false --conf \"spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\" --conf spark.hadoop.abc.def=xyz --conf spark.hive.abc=xyz path_to_your_python_file```\n",
    "4. To avoid errors, copy the command above into a text editor so that everything is on one line\n",
    "5. Replace ```num_executors``` with a number such as ```4``` for a start. Press enter to run the program.\n",
    "6. As the program runs, take note how many executors are created, note the running time for Spark funciton only\n",
    "7. Now, increase the ```num_executors``` by 2 or 4 and run the program again. See if you notice reduction in running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c9666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
