{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36260f58",
   "metadata": {},
   "source": [
    "# Hello Spark\n",
    "In this tutorial, you will run our first Spark application in Python. Thus, this our Spark Driver process.\n",
    "We will cover the following items:\n",
    "- Build a SparkSession\n",
    "- Use the high level API DataFrame to read a CSV file\n",
    "- Compare running time between Spark and Pandas when reading this large CSV file\n",
    "- Explore basic Spark DataFrames functionality\n",
    "- Explore the Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d11eb1",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Import required Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde5a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "import time\n",
    "from functools import wraps\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7ed47",
   "metadata": {},
   "source": [
    "# Create a SparkSession\n",
    "As we mentioned during the lecture, there is one main way to interact \n",
    "with Spark and thats using the SparkSession object. Therefore, the very first when writing a \n",
    "Spark application is to instantiate a SparkSession object. A SparkSession can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. There\n",
    "can only be one SparkSession per JVM. When creating a SparkSession, there are several options/parameters you can pass. For instance, pass number of cores. We can use ```*``` to instruct Spark to use all cores available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c3fa93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Hello Spark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2f3a2",
   "metadata": {},
   "source": [
    "# Read Data into a Spark DataFrame\n",
    "We will get into details about the SparkDataFrames API but for now, its enough for you \n",
    "to understand that we can use the SparkSession to read data from different sources such as CSV as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3981ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please use the activity_log_raw.csv  file here\n",
    "large_csv = \"/Users/dmatekenya/wbg/cuebic-raw-data/processed/input.csv\"\n",
    "df = spark.read.csv(cdr_file)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02331e25",
   "metadata": {},
   "source": [
    "# Compare running time for Spark and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d313e5",
   "metadata": {},
   "source": [
    "**EXERCISE-1**: Complete the parts which  says \"YOUR CODE HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71db01d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def timefn(fn):\n",
    "    \"\"\"\n",
    "    Function for recording running time of a function\n",
    "    \"\"\"\n",
    "    @wraps(fn)\n",
    "    def measure_time(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = fn(*args, **kwargs)\n",
    "        t2 = time.time()\n",
    "        print(\"@timefn:\" + fn.__name__ + \" took \" + str(t2 - t1) + \" seconds\")\n",
    "        return result\n",
    "    return measure_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750f6cb2",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "@timefn\n",
    "def load_big_csv_with_spark(big_csv=None):\n",
    "    \"\"\"\n",
    "    A simple function which loads a CSV file using Apache Spark and\n",
    "    then counts how many rows are in the file\n",
    "    \"\"\"\n",
    "    # create a Spark Session here\n",
    "    spark = YOUR CODE HERE\n",
    "    # read the CSV  file int Spark DataFrame\n",
    "    df = YOUR CODE HERE\n",
    "    # Get the number of rows in the dataset using the count() function\n",
    "    cnt = YOUR CODE HERE\n",
    "    print('Number of rows in big CSV file: {:,}'.format(YOUR CODE HERE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b24e7909",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "@timefn\n",
    "def load_big_csv_with_pandas(big_csv=None):\n",
    "    \"\"\"\n",
    "    Use pandas library to load the large CSV\n",
    "    \"\"\"\n",
    "    # Read CSV as a pandas DataFrame (df) here\n",
    "    df = YOUR CODE HERE\n",
    "    \n",
    "    # Get the total number of rows\n",
    "    cnt = YOUR CODE HERE\n",
    "   \n",
    "    print('Number of rows in big CSV file: {:,}'.format(YOUR CODE HERE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64becb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call the two functions above here\n",
    "YOUR CODE HERE\n",
    "YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d196630d",
   "metadata": {},
   "source": [
    "# Exploring DataFrames in Spark\n",
    "We can do the similar things we did with pandas dataframes in order to explore the data.\n",
    "- We can use the `printSchema()` function to check column data types as well as what kind of columns we have.\n",
    "- Also, you can use the the same `columns` function to get a list of all columns\n",
    "- We can use `head()` function just like in pandas to get the top `n` observations from the data. Note that you can use n within the brackets in the head function to specify the number of rows you want to see.\n",
    "- Count the number of rows in the dataframe using the `count()` function\n",
    "- Get number of unique elements using `distinct()` command. If you want number of unique elements based on a single column. You can first select the column using the sysntax `df.select(\"column\").distinct().count()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a5173",
   "metadata": {},
   "source": [
    "## Exercise-2: Explore the data\n",
    "1. Use ```head()``` function to view the first 5 observations\n",
    "2. Check column data types\n",
    "3. How many unique categories are there for the STATUS variable?\n",
    "4. Using the documentation for [Pyspark DataFrame](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) find a function to get a sample of the data. Get a sample of 10% of the data and convert it to a pandas dataframe using `toPandas()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5812da",
   "metadata": {},
   "source": [
    "#  Explore Spark UI\n",
    "In order to explore the Spark UI, we will run a function that take time to run. \n",
    "For this, let run the ```summary()``` funciton. Once you call the function, open the spakr UI by going to this *localhost/4040*. Once there, explore the different tabs:\n",
    "- **Jobs:** On the Jobs tab, you can check the  status of your current job. You can see the exact functions such as count that  Spark is running\n",
    "- **Executors:** In this tab, you can check how cores is Spark using and how tasks are being run in each core\n",
    "- **Stages:**. You can at which stage the job is running, how many tasks have completed\n",
    "- **Environment:** Its important to see which environment variables Spark is using, you can check that using this tab.\n",
    "- **Storage, SQL:** Explore these tabs to see what information they contain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d160beff",
   "metadata": {},
   "source": [
    "**EXERCISE-3:** If the summary job is taking too long, please kill it using the ```kill`` function in Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f4998",
   "metadata": {},
   "source": [
    "# EXERCISE-4:\n",
    "Running spark using ```spark-submit``` and comparing Spark Running time based on the number of executors assigned to Spark. Unfortunatel, in Jupyter notebook, setting the number of executors isnt working well and so we will have to do it in terminal to explore this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d1d7f",
   "metadata": {},
   "source": [
    "## Step-1: Identify location of your Spark installation. \n",
    "- Run code below to note down the base folder of you Spark\n",
    "- Make sure you identify the root of the Spark folder\n",
    "- On the terminal, navigate to that folder using cd and then navigate to the ```bin``` folder\n",
    "- If you run the ```ls``` command while in the ```bin``` folder, you should see a ```spark-submit``` executable\n",
    "- ```spark-submit``` is used to submit standalone Spark applications  to a cluster but we can use it in local mode too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7074e100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dmatekenya/spark-3.0.0-bin-hadoop2.7/python/pyspark/__init__.py'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run code below to note down the base folder of you Spark\n",
    "# Make sure you identify the root of the Spark folder\n",
    "import pyspark\n",
    "pyspark.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1baf90",
   "metadata": {},
   "source": [
    "## Step-2: Create a ```.py``` file in VS Code or any text editor\n",
    "- Copy all the necessary imports and add them at the top of the Python file\n",
    "- Next, add this line of code as its always required when running Python scripts: \n",
    "```if __name__ == \"__main__\":```\n",
    "- Copy the code from just after this heading: ```Compare running time for Spark and Pandas``` to before the heading: ```Exploring DataFrames in Spark```. And paste the code underneath the statement above.\n",
    "- Make sure your Python file has no errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a932e12",
   "metadata": {},
   "source": [
    "## Step-3: Run the Spark Application\n",
    "1. Note the full path of your Python file\n",
    "2. On the terminal, navigate to the Spark ```cd```\n",
    "3. Within the Spark folder, run this command:\n",
    "```./bin/spark-submit --name \"Hello Spark\" --master local[num_executors] --conf spark.eventLog.enabled=false --conf \"spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\" --conf spark.hadoop.abc.def=xyz --conf spark.hive.abc=xyz path_to_your_python_file```\n",
    "4. To avoid errors, copy the command above into a text editor so that everything is on one line\n",
    "5. Replace ```num_executors``` with a number such as ```4``` for a start. Press enter to run the program.\n",
    "6. As the program runs, take note how many executors are created, note the running time for Spark funciton only\n",
    "7. Now, increase the ```num_executors``` by 2 or 4 and run the program again. See if you notice reduction in running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c9666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
