{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b34b4d0",
   "metadata": {},
   "source": [
    "# Spark APIs\n",
    "## Introdution\n",
    "One of the advantages of Spark is that although it is written in Scala, it provides several langauge APIs: you can work with using Scala, Java, Python or R. Withing each programming environment, Spark provides a common way to interact with its functionality through its APIs. At a lower level, you can use the RDD API which is the foundational API upon which other higher level APIs are built on. These higher level API's are structured APIs suitable for dealing with data. All the structured APIs are built on top of Spark-SL engine. Spark provides the following structured APIs:\n",
    "- Spark SQL\n",
    "- DataFrame\n",
    "- pandas API on Spark (available in Python)\n",
    "- Datasets (only available in Scala and Java)\n",
    "In addition to these core data APIs, Spark provides other API such as the MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.\n",
    "\n",
    "## Learning outcomes\n",
    "In this tutorial, you will explore Spark RDDs, DataFrame, SQL to appreciate how they differ. At the end of the tutorial, you should be able to:\n",
    "- Write Spark program using RDDs\n",
    "- Write MapReduce program style in Spark using RDDs\n",
    "- Use Spark DataFrame API\n",
    "- Use pandas API on Spark\n",
    "- Write a Spark-SQL queries\n",
    "- Appreciate the difference between RDD as a low-level API and Spark structured API's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d5d8d-0607-46ca-903a-76f2f82e8c48",
   "metadata": {},
   "source": [
    "## Python setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1d9b8d-59e7-400a-b427-701780ce2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python setup\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf01fa1-c3cb-4cd0-afaf-125459dc563b",
   "metadata": {},
   "source": [
    "## Inputs setup\n",
    "Lets provide paths to input files we will use. \n",
    "Its a good practice to create these as global variables. Also, use Python module ```Path``` from pathlib to manage file paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa670af-ffd2-4b5c-9c6d-3ef9edf713ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altenatively, you can put a full-path to wheree your data is located like below\n",
    "# DATA_DIR = Path(full-path-folder-where-you-are-keeping-data)\n",
    "DATA_DIR = Path().cwd().parents[0].joinpath(\"DATA\")\n",
    "\n",
    "\n",
    "# Path to any large CSV file (e.g., activity_log_raw,csv)\n",
    "LARGE_CSV = Path().cwd().parents[2].joinpath(\"WBG-LOCAL/MADAGASCAR-POV-MAPPING/data/input/census/data/ResidentIBEIPM.csv\")\n",
    "\n",
    "# path to hh_data.txt\n",
    "HH_DATA = DATA_DIR.joinpath(\"raw\", \"hh_data.txt\")\n",
    "\n",
    "# word_count_files folder\n",
    "# in your data folder, create a word_count_files folder\n",
    "WORD_COUNT = DATA_DIR.joinpath(\"raw\", \"word_count_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9064b2a-2aec-4456-a3e1-ecd322a2b698",
   "metadata": {},
   "source": [
    "## The Data Science Task\n",
    "### Description of the data\n",
    "In order to explore these Spark API, we use the [following dataset](). The data is from a population and housing census of some country ```X```  not identified for privacy reasons although this data is a very small subset of the actual data. \n",
    "Each row in the data represent a single individual in the population. \n",
    " A summary of column description is provided below:\n",
    "- **Geographic identifiers:** province, region, district represented by ```prov_id, reg_id, dist_id and adm4``` (administrative region level 4-which comes after district)\t respectively. Also, these same variables are embsedded in the ```hh_id``` column which represents unique id of each household. \n",
    "- **urban_rural:** A classification iof whether this person lived in urban or rural area\n",
    "- **Sex**. ```P05```==>[1\t- Masc 2\t- Fém]\n",
    "\n",
    "- **P03:** whether the person is the head of the household, wife. child etc==>[0- Chef de Ménage (CM) 1- Conjoint(e) (CJ) 2-\tFils/Fille3-\tPère/Mère 4-\tBeau-Père/Belle-Mère 5-\tBeau-Fils/Belle-Fille 6-\tPetit fils/Petite-fille\n",
    " Autre Proche du CM 8- Autre proche du CJ 9 -Sans lien de parenté]\n",
    "\n",
    "- **Age:**. Person's date of birth is given by column ```P07M``` (month of birth), ```P07A``` (year of birth) and ```P08``` (age)\n",
    "- **Marital status:** ```P28``` (whether the person is married or not)==>[1-\tCélibataire, 2-\tMarié(e), 3-\tDivorcé(e)/Séparé(e), 4-\tVeuf(ve)\n",
    "] while ```P29``` (age at marriage).\n",
    "- **School attendance:** ```P21``` ==>[0 N'a Jamais fréquenté 1-A\tfréquenté 2- Fréquente actuellement]\n",
    "- **Highest school level attended:**```P22N``` (see screenshot below for interpretation of values)\n",
    "- **Whether the person worked or not:** ```P23```==> [1-\t0ccupé 2-\tChômeur 3-\tEn quête du 1er emploi 4-\tMénagère 5-\tElève/Etudiant 6-\tRetraité 7- lncapacité à travailler 8- Autre]\n",
    "\n",
    "### Description of the data science task\n",
    "We would like to find out the following from the data:\n",
    "1. What is the mean age in the country?\n",
    "2. Which province has the largest population\n",
    "3. Whats the mean household size in the country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ace71c-8c1f-4176-ae2a-06f85696028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../DOCS/images/P22N.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773d08f-68f3-425d-a26d-1136eaba1f92",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RDDs\n",
    "````Most of these notes are shamelessly copied from here; please visit the site for more indepth discussion````\n",
    "\n",
    "We learned during the lectures that every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. \n",
    "Although Spark provides high level APIs such as DataFrames and SQL, all these are built on top the lowe level abstraction called resilient distributed dataset (RDD). Although you will not often work with RDDs, \n",
    "its still important to understand the basics of how they work as more often than not, functionality in the high level APIs will become limited and you will need to work with RDDs.\n",
    "\n",
    "### What are RDDs and how do create them\n",
    "An RDD is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are considered as the main/first abstraction provided by Spark. There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat. \n",
    "\n",
    "### Spark shared variables\n",
    "A second abstraction in Spark is shared variables that can be used in parallel operations. By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: **broadcast variables**, which can be used to cache a value in memory on all nodes, and **accumulators**, which are variables that are only “added” to, such as counters and sums."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad3b94-ad38-458f-a457-6c08df5c6d3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initializing Spark\n",
    "When working with RDD's, unlike with DataFrames, we use a **SparkContext** object, which tells Spark how to access a cluster. To create a SparkContext you first need to build a SparkConf object that contains information about your application. However, we can stitll create a **SparkContext**  object from SparkSession. \n",
    "\n",
    "When we initialize Spark, we need to provide several parameters as follows:\n",
    "- ```appName```. Any name you would like tomgive to the spark application\n",
    "- ```master()```. In a cluster setup, there is always one computer node which acts as a controller or master node and thats where the driver program runs. Spark needs to know the IP adreess of this node. \n",
    "Also, in a cluster, as you saw, every node has an IP address as thats how the nodes communicate with each other. \n",
    "When running in local mode, the driver program and Spark executors all run on the same node (or the local host). We tell Spark by using string: ```local[num_cores]``` \n",
    "and we can specify how many cores to use on your machine. For example, ```local[4]``` or ```local[*]``` to use all cores on your machine. \n",
    "The IP adress for local host is often ```127.0.0.1```: in some cases you want to tell Spark the exact adress.\n",
    "- ```config()```. When we initialize Spark, we can also pass many configurations through the ```config()``` property. \n",
    "This is to provide run-time settings such as how much memory to give to the driver, how many cores etc. Please see [Spark configuration properties](https://spark.apache.org/docs/latest/configuration.html) for the full list of configuration. For example, you can tell Spark to give the driver program 8GB of RAM like this: config(\"spark.driver.memory\", \"8g\")\n",
    "\n",
    "#### Viewing current Spark configurations and settings\n",
    "Its important to view the current settings as in some cases you may be getting an error because Spark doesnt have access to enough resources. \n",
    "Given a SparkSession object called ```spark```, \n",
    "you can get confihiurations by invoking the SparkConfig object through the SparkContext object like this: ```spark.sparkContext.getConf().getAll()```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa74cd-2efc-499d-b587-344df50bad20",
   "metadata": {},
   "source": [
    "#### Initialize Spark with default configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a0c22c-411f-479f-9196-b163981a28bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/15 13:52:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/15 13:52:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# create a SparkSession which is enough to access DataFrame, Datasets and SQL API's\n",
    "spark = SparkSession.builder.appName(\"intro\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# We create SparkContext object which we need to access the RDD API\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812f0afe-8dbc-4d59-b270-d98e837b027b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\turban_rural\\thh_id\\tP03\\tP05\\tP07M\\tP07A\\tP08\\tP21\\tP22N\\tP23\\tP28\\tP29'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_hh = sc.textFile(str(HH_DATA))\n",
    "rdd_hh.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09dfd41-03ae-46c8-ab29-554081d89446",
   "metadata": {},
   "source": [
    "#### Initialize Spark with custom configurations\n",
    "When we set Spark configurations this way, we are doing it in runtime and we override all other Spark settings and configurations. \n",
    "Its worth mentioning that in some cases (at least it happens on my MacBook), \n",
    "a Spark application fails to pick up these new settings and still runs with default configs. So, where are these default configurations?\n",
    "\n",
    "##### Spark default configurations\n",
    "If you set configurations in a Spark application they seem not to work, you need to change the Spark default configurations. \n",
    "In the Spark installation folder, you will find a config folder where you can make changes to the ```spark-defaults.conf```\n",
    "to add the configurations we are providing below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d51ac-3744-49cf-96a9-3958c8a685a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"../DOCS/images/spark-confs.png\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e65c8c-0963-496e-aa83-9e7b4218c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.appName(\"intro\").master(\"local[*]\")\\\n",
    "#                     .config(\"spark.executor.cores\", '3') \\\n",
    "#                     .config(\"spark.executor.memory\", '8g')\\\n",
    "#                     .config('spark.driver.maxResultSize', '6g')\\\n",
    "#                     .getOrCreate()\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c6ae3-4881-43ea-9cdc-a5e5f5b3f57c",
   "metadata": {},
   "source": [
    "<font color='blue'>**EXERCISE-1: EXPLORE SPARK CONFIGS** </font> \n",
    "1. Print out spark current configs using ```spark.sparkContext.getConf().getAll()```\n",
    "2. Explore the configuration ```spark.driver.maxResultSize```. What does it control?\n",
    "3. Whats the difference between ```spark.executor.instances``` and ```spark.executor.cores```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de65ec49-1fce-49ce-9782-e00fe639eb78",
   "metadata": {},
   "source": [
    "### Creating RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ad1cf-fc11-4bc7-9a50-9546a47745cf",
   "metadata": {},
   "source": [
    "#### RDDs from Python objects on the driver program\n",
    "Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program in Python.\n",
    "The elements of the collection are copied to form an RDD that can be operated on in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a2cad-9735-4eb3-aa83-6ae0e0ad6895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the parallelize function to create an RDD from Python objects\n",
    "# however, we need a SparkContext object to create RDD and we can create  it from SparkSession\n",
    "data = [(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)]\n",
    "colnames = [\"name\", \"age\"]\n",
    "rdd_data = sc.parallelize(data)\n",
    "# Use map and reduceByKey transformations with their lambda \n",
    "# expressions to aggregate and then compute average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6201360-bf18-4a79-b0c3-9e415dac8647",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490eff91-c424-4952-a7b8-2c4d2665c423",
   "metadata": {},
   "source": [
    "#### RDDs from external data sources \n",
    "In practice, we often ingest data from external sources. PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "\n",
    "Text file RDDs can be created using SparkContext’s textFile method. This method takes a URI (full path to the file) for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and \n",
    "reads it as a collection of lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a61c2c-8de9-49ad-96ff-96f1601a7425",
   "metadata": {},
   "source": [
    "<font color='blue'>**EXERCISE-2: LOADING TEXT FILES WITH SPARKCONTEXT** </font> \n",
    "\n",
    "1. Read through the documentation for the method ```sc.textFile()```. \n",
    "2. Compared to how we read data with pandas DataFrame and/or in R, what are some limitations of this method? Mention at least 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3dce6b-e050-4016-9c1c-b3eac79a07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "rdd_from_file = sc.textFile(str(HH_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b652c6e0-ca11-4f6c-af33-3976adcebfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_from_file.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d2121-10c0-415c-bea8-8fe6fc52382f",
   "metadata": {},
   "source": [
    "### Working with RDDs\n",
    "Once we have loaded our external dataset ito an RDD, what can we do with it? \n",
    "Unlike a DataFrame, RDD's dont provide many already made data functions such a aggregation. Instead, you have to use low level methods such as map to create such functions for yourself. \n",
    "In order to explore the available methods on a Spark RDD object, refer to the [Pyspark RDD API documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a874c66-6524-4824-8a2d-cb8e3e2d8baf",
   "metadata": {},
   "source": [
    "<font color='blue'>**EXERCISE-3: USEFUL METHODS FOR DATA PROCESSING/EXPLORATION/MANIPULATION ON THE SPARK RDD OBJECT** </font> \n",
    "\n",
    "Clearly, the Spark RDD was not designed to provide the type of functionality we are used to in Pandas DataFrames and/or DataFrames in R. \n",
    "However, there is still some reasonable functionality to enable quick data exploration. \n",
    "1. Provide any such functions whi the RDD obkect has which are useful for data wrangling? Provideat at least 3 methods\n",
    "2. For each method mentioned above, use it on the them RDD defined from ```hh_data.txt``` and report the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ddff90-973d-4581-99d5-6ea878122f2f",
   "metadata": {},
   "source": [
    "### Compute national mean age from scratch from RDD\n",
    "1. Split each line in the RDD into separate columns\n",
    "2. Make sure we skip the header column\n",
    "3. We need to convert the string into numbers\n",
    "4. Use RDD [map](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html#pyspark.RDD.map) and [reduce](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html#pyspark.RDD.reduce) functions to compute mean.\n",
    "Other function which is also useful is ```filter```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c064c9a-d298-47b9-a7bc-37c396bb5587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_line(line):\n",
    "    \"\"\"\n",
    "    Split the line which has a single string\n",
    "    into a list where each element represents a\n",
    "    column\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the line\n",
    "    col_items = line.split(\"\\t\")\n",
    "    \n",
    "    # Do some extra processing\n",
    "    # Because we know age is in col 7, we can only\n",
    "    # return age and also convert to numeric\n",
    "    try:\n",
    "        return float(col_items[7])\n",
    "    except:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ed8a1-7963-4036-a72e-ef8e8155999f",
   "metadata": {},
   "source": [
    "# The lines which are chained can be split and run separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a2ce7-445b-4a6a-9b07-676e00639121",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_from_file_split = rdd_from_file.map(split_line)\n",
    "\n",
    "# To check that its running okay, use collect()\n",
    "# collected = rdd_from_file_split.collect()\n",
    "\n",
    "rdd_from_file_split_nums = rdd_from_file_split.filter(lambda x: x != 'NaN')\n",
    "#collected_nums = rdd_from_file_split_nums.collect()\n",
    "#print(collected_nums[:5])\n",
    "# Now run reduce\n",
    "\n",
    "rdd_sum = rdd_from_file_split_nums.reduce(lambda x, y: x + y)\n",
    "\n",
    "obs_cnt = rdd_from_file_split.count()\n",
    "avg_age = rdd_sum/obs_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc35da-153f-4f5f-abb3-d123f8a594bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the header\n",
    "# Split lines and keep only age and convert it to numeric\n",
    "rdd_from_file_split = (rdd_from_file\n",
    "                .map(split_line)\n",
    "                .filter(lambda x: x != 'NaN'))\n",
    "\n",
    "# Apply reduce to get sum of al numbers\n",
    "rdd_sum = rdd_from_file_split.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Get number of elements in list using count() and then get mean\n",
    "obs_cnt = rdd_from_file_split.count()\n",
    "avg_age = rdd_sum/obs_cnt\n",
    "\n",
    "\n",
    "# Check out output\n",
    "print('National level aveerage age: {}'.format(int(avg_age)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6499695-d0d7-4bee-af95-137518b7c432",
   "metadata": {},
   "source": [
    "<font color='blue'>**EXERCISE-4: WHAT IF WE WANTED TO COMPUTE AVERAGE AGE FOR EACH DISTRICT?** </font> \n",
    "1. Use toy example below to learn how to compute average when we have keys.\n",
    "2. Next, use the same strategy to compute and report mean age by district.\n",
    "    - Recall that district code is embedded in the hh_id columnn. Refer to data description above\n",
    "3. Save the output into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f745d8a-22ce-4819-b99c-759598596ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can use the parallelize function to create an RDD from Python objects\n",
    "# however, we need a SparkContext object to create RDD and we can create  it from SparkSession\n",
    "data = [(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)]\n",
    "colnames = [\"name\", \"age\"]\n",
    "rdd_data = sc.parallelize(data)\n",
    "# Use map and reduceByKey transformations with their lambda \n",
    "# expressions to aggregate and then compute average\n",
    "\n",
    "ages_rdd = (rdd_data\n",
    ".map(lambda x: (x[0], (x[1], 1)))\n",
    ".reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    ".map(lambda x: (x[0], x[1][0]/x[1][1])))\n",
    "\n",
    "# you can use collect() function on RDD to bring all the data on a single core and \n",
    "# look at it \n",
    "ages_rdd_list = ages_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ae12c-2cbf-4a78-a205-5a3fc68aa1cb",
   "metadata": {},
   "source": [
    "### MapReduce type of computations in with Spark RDD's\n",
    "The Spark RDD has several **map** and **reduce** which are similar in style to the MapReduce type of functions \n",
    "which we write with Hadoop  MapReduce but they are not exactly the same. We expore some of the functions below:\n",
    "- map\n",
    "- flatmap\n",
    "- mapValues\n",
    "- mapPartitions\n",
    "- reduce\n",
    "- reduceByKey\n",
    "- reduceByKeyLocally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24321516-b814-490e-b963-7ef1d1f76b31",
   "metadata": {},
   "source": [
    "#### Word count using Spark\n",
    "Lets do the classical word-count with spark to explore differences in how the different map functions work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1ec5b32-a3b3-4c80-b8ed-4b53eb1c3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = spark.sparkContext\n",
    "words  = ['hadoop is fast',  'hive is sql on hdfs', 'spark is superfast',  \n",
    "          'spark is awesome', 'spark is faster than hadoop', 'spark is very fast']\n",
    "rdd_data = sc.parallelize(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719190c0-1c7c-4d4a-a402-a0ad2be5cd84",
   "metadata": {},
   "source": [
    "##### First, lets use ```flatmap``` \n",
    "Note that with ```reduceByKey``` we dont need to do the groupByKey step required in MapReduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae2a6f96-2b7e-41bc-90a1-375283a1aedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of flatmap \n",
      "\n",
      "['hadoop', 'is', 'fast', 'hive', 'is', 'sql', 'on', 'hdfs', 'spark', 'is', 'superfast', 'spark', 'is', 'awesome', 'spark', 'is', 'faster', 'than', 'hadoop', 'spark', 'is', 'very', 'fast']\n",
      "\n",
      "Results of map on flatmap results \n",
      "\n",
      "[('hadoop', 1), ('is', 1), ('fast', 1), ('hive', 1), ('is', 1), ('sql', 1), ('on', 1), ('hdfs', 1), ('spark', 1), ('is', 1), ('superfast', 1), ('spark', 1), ('is', 1), ('awesome', 1), ('spark', 1), ('is', 1), ('faster', 1), ('than', 1), ('hadoop', 1), ('spark', 1), ('is', 1), ('very', 1), ('fast', 1)]\n",
      "\n",
      "Results of reduceByKey \n",
      "\n",
      "[('faster', 1), ('hive', 1), ('hdfs', 1), ('spark', 4), ('awesome', 1), ('very', 1), ('fast', 2), ('sql', 1), ('hadoop', 2), ('superfast', 1), ('is', 6), ('than', 1), ('on', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Split lines into separate words\n",
    "fm = rdd_data.flatMap(lambda line:line.split(\" \"))\n",
    "\n",
    "# View flatmap results\n",
    "print('Results of flatmap \\n')\n",
    "print(fm.collect())\n",
    "\n",
    "# create key-value pairs\n",
    "fm_kv = fm.map(lambda x: (x,1))\n",
    "print()\n",
    "print('Results of map on flatmap results \\n')\n",
    "print(fm_kv.collect())\n",
    "\n",
    "# Now reduce to get totals by key\n",
    "wc = fm_kv.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "print()\n",
    "print('Results of reduceByKey \\n')\n",
    "print(wc.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390e36f-4381-47af-9722-c59767fbffed",
   "metadata": {},
   "source": [
    "<font color='blue'>**EXERCISE-5: WORD-COUNT WITH EXACT MAPREDUCE STEPS** </font> \n",
    "\n",
    "If you really wanted to replicate the MapReduce computations in Hadoop, you can do the following\n",
    "1. Grab the key-value pairs RDD from above\n",
    "2. Group them by key using ```groupByKey()```\n",
    "3. Import functions we will use from Python built-in modules.\n",
    "    - From the ```operator``` module import function ```add```. What does this little func do?\n",
    "    - From the ```functools``` import ```reduce```. Recall this function from our functional programming lesson.\n",
    "4. Use the ```mapValues()``` function together with ```reduce``` and ```add``` to perfom reduction on the contents of the lis from each key. \n",
    "\n",
    "> You can just complete the code in cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ad726-e250-458e-9a73-dec9cae47a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the above mentioned functions here\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "\n",
    "\n",
    "## Take the key_value pairs and group them by key\n",
    "# create key-value pairs\n",
    "fm_kv_grp_bykey = None\n",
    "\n",
    "print()\n",
    "print('Results of groupByKey \\n')\n",
    "print(fm_kv_grp_bykey.collect())\n",
    "\n",
    "# We need to use mapValues on each key-value pair to get list of values\n",
    "# and then use Python reduce and add on that list\n",
    "fm_kv_grp_bykey_list = None\n",
    "\n",
    "print()\n",
    "print('Results of groupByKey \\n')\n",
    "print(fm_kv_grp_bykey_list.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c904459-f4f4-46bb-a01e-d07cf6df80e4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "    We did all of the above tedious word counting stuff just for learning, otherwise, it is much easier to get those type of stats from an RDD using built-in Spark RDD functions as shoen below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb25f87-4316-4916-bc42-0584da7b2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load the text files for word_count\n",
    "word_count_rdd = sc.textFile(str(WORD_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c7957-59ab-4ecc-821a-7b994526cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = word_count_rdd.flatMap(lambda line:line.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656286a-f2ae-49bd-8cc7-dacd14e9f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [word_count[x:x+3] for x in range(0, len(word_count), 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841979a9-d3d6-42e8-8ab4-311ac7df597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c3c94-8042-48ee-bcc7-27d94581ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "\n",
    "n = 3\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "\n",
    "for grams in sixgrams:\n",
    "  print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd954f6-42b8-46d4-9c1a-cdd687fe1beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can countByKey after using flatMap() and map()\n",
    "word_count = word_count_rdd.flatMap(lambda line:line.split(\" \")).map(lambda x: (x,1)).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80d948-b80c-4cb2-946f-ee986f715533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MapReduce style\n",
    "word_count = word_count_rdd.flatMap(lambda line:line.split(\" \")).map(lambda x: (x,1)).reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f99e43-e226-4ab8-936a-0c32214decfa",
   "metadata": {},
   "source": [
    "### RDD from DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c385e-ad1e-4fcf-8368-ca37e2d1f9f8",
   "metadata": {},
   "source": [
    "# Loading external data into RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b63c656e-a71a-4700-913e-86cbe0566412",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.csv(str(HH_DATA), header=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7df3fd-990e-4a9b-8e74-5a1f35a119eb",
   "metadata": {},
   "source": [
    "## Spark DataFrames \n",
    "In this course we will mostly use the Spark DataFrame API because its the most convinient for Daata Analysts, ML Engineers, \n",
    "Data Scientists and in some cases Data Engineer. We will delve deeper into the DataFrame API in other notebooks. In this tutorial, the idea is just to show how it differs from RDD and SQL API's. \n",
    "\n",
    "Just like RDD's, there are several ways to create a Spark DataFrame but here, we will load a DataFrame from an external file. With DataFrame, answering the analysis questions below is straightfoward:\n",
    "1. What is the mean age in the country?\n",
    "2. Which province has the largest population\n",
    "3. Whats the mean household size in the country?\n",
    "\n",
    "In working with Spark DataFrames, you can seamlessly work with other Python objects and Python packages such as pandas. Everytime a large dataframe has been reduced to a small size, you can ```collect``` it as pandas DataFrame and work with it on the driver program as a non-distributed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3052fe-7767-4d5b-b8fb-54a0ccb764f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_hh = spark.read.csv(str(HH_DATA), header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c42b52-f2bb-4431-a9b2-51ceaf251201",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_hh.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e0f02-02ce-4830-8796-4b45bdf6f054",
   "metadata": {},
   "source": [
    "### Compute national mean age from scratch from Spark DataFrame\n",
    "We simply call the function ```avg``` on the column of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30497f0a-e5ef-448e-8a25-7545d64f66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col, udf\n",
    "sdf_hh.select(avg(col('P08'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22009d2-6c14-405a-af55-f94346fc4f16",
   "metadata": {},
   "source": [
    "### Which province has the largest population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efaafee-319a-45e0-a125-a7ec7a474851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add prov_id to the dataframe\n",
    "sdf_hh2 = sdf_hh.withColumn('prov_id', udf(lambda x: x[0])('hh_id'))\n",
    "pdf_prov_mean_age = sdf_hh2.groupby('prov_id').count().toPandas()\n",
    "largest_pop_prov = pdf_prov_mean_age.sort_values(by='count', ascending=False).iloc[0]['prov_id']\n",
    "largest_pop = pdf_prov_mean_age.sort_values(by='count', ascending=False).iloc[0]['count']\n",
    "\n",
    "print('Province with ID: {} has the largest population of {:,} people'.format(largest_pop_prov, largest_pop))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d326a51d-5cb7-4562-8785-81314ed6e53a",
   "metadata": {},
   "source": [
    "### Whats the mean household size in the country? Which province has the highest mean hh_size?\n",
    "A household is provided by the column ```hh_id```. With this column, we can group the persons into households and calculate household size for each household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39bb56b-0bcf-4fcb-b81f-e0a3f7f7a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_hhs = sdf_hh2.groupby('hh_id').count().toPandas()\n",
    "# pdf_hhs['prov_id'] = pdf_hhs.hh_id.apply(lambda x: x[0])\n",
    "pdf_hhs.rename(columns={'count': 'hh_size'}, inplace=True)\n",
    "pdf_hhs_prov_stats = pdf_hhs.groupby('prov_id').agg({'hh_size':'mean'}).reset_index()\n",
    "\n",
    "national_avg_hh_size = pdf_hhs.hh_size.mean()\n",
    "prov_with_largest_hh_size = pdf_hhs_prov_stats.sort_values(by='hh_size', ascending=False).iloc[0]['prov_id']\n",
    "largest_hh_size = pdf_hhs_prov_stats.sort_values(by='hh_size', ascending=False).iloc[0]['hh_size']\n",
    "print('Province with ID: {} has the largest average HH-SIZE of {:.2f} compared to national average of {:.2f}'.format(prov_with_largest_hh_size,\n",
    "                                                                                                                     largest_hh_size,\n",
    "                                                                                                                    national_avg_hh_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf5247-72d1-4fa4-89f3-c79b126374ec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Clearly, working with Spark DataFrames is much much easier when compared to working with RDD's for common data science tasks. \n",
    "However, the keyword here is common: in almost all of the large scale *Big Data* projects where I used Spark, \n",
    "I encountered scenarios where I had no choice but to use RDD because the functionality I was lookig for wasnt working well with DataFrame. So, thats the main reason we still bother about RDD's.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143074a-389b-482d-8a01-45a021c07fce",
   "metadata": {},
   "source": [
    "<font color='blue'>**EXERCISE-6: COMPUTE THE FOLLOWING USING THE SPARK RDD** </font> \n",
    "\n",
    "Instead of using DataFrame which is straightfoward, please use RDD to compute the following from the ```hh_data.txt``` data.\n",
    "1. **Largest population**. Which province has the largest population and whats the population?\n",
    "2. **Household size**. Whats the average household size? The province with largest average household size and the corresponding average household size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca608fb-d455-4ea5-951c-85cba19f1521",
   "metadata": {},
   "source": [
    "## Pandas API on Spark\n",
    "Recently, Spark introduced a new API in allowing users to run code directly as they do in Pandas. \n",
    "Please take some time to explore this API using the documentation [here](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html). Although this makes life very easy since onc edoesnt have to learn Spark APIs function and commands, such simplicty often comes at a cost such as failure to fully control how the DataFrames are partitioned, loss of speed because Spark is iincorporating with Pandas and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395feab-f68f-4af6-8673-6f57e63f962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77784d-383b-495e-a7ed-2986d6864acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_pandas_df = ps.read_csv(str(HH_DATA), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03667129",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "Spark SQL is a foundational component of Apache Spark that integrates relational processing with Spark’s functional programming API.Spark SQL lets Spark programmers leverage the benefits of faster performance and relational programming (e.g., declarative queries and optimized storage), as well as call complex analytics libraries (e.g., machine learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e77c6e-f5f4-4f3e-898b-dee2182de1b2",
   "metadata": {},
   "source": [
    "### Run SQL queries programmatically on DataFrames\n",
    "The sql function on a SparkSession object enables applications to run SQL queries programmatically and returns the result as a DataFrame. \n",
    "\n",
    "In SQL, ```views``` are kind of virtual tables. A view also has rows and columns as they are in a real table in the database. We can create a view by selecting fields from one or more tables present in the database. In Spark, we need to create a view from the DataFrame before we can run SQL commands.\n",
    "\n",
    "# The SQL API: interact with a CSV file read as DataFrame with SQL commands\n",
    "As mentioned, Spark allows you to read (ee.g., a CSV file) in data as DataFrame but you can interact with it using good old SQL commands. The following steps are required in order to \n",
    "1. **Create a DataFrame as required:**. In our case, we will read from external source.\n",
    "2. **Create a table  view:**. Views are a special version of tables in SQL. They provide a virtual table environment for various complex operations. You can select data from multiple tables, or you can select specific data based on certain criteria in views. It does not hold the actual data; it holds only the definition of the view in the data dictionary (you will learn more about this in the Database course).\n",
    "\n",
    "Once you have a temporary view, you can issue SQL queries using Spark SQL. These queries are no different from those you might issue against a SQL table in, say, a MySQL or PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0161eb3-68ac-4327-a29f-385ccd7cf6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/10 06:59:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59a5ffde-0c86-4910-a339-2c0762c6ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.csv(str(HH_DATA), header=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79bc966-9cfc-447e-8710-7acdf88ab7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/10 07:00:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , urban_rural, hh_id, P03, P05, P07M, P07A, P08, P21, P22N, P23, P28, P29\n",
      " Schema: _c0, urban_rural, hh_id, P03, P05, P07M, P07A, P08, P21, P22N, P23, P28, P29\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/dmatekenya/TEACHING-LOCAL/BDA-with-Python/DATA/raw/hh_data.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(_c0='0', urban_rural='1', hh_id='11101101010011066020020002', P03='0', P05='1', P07M='10', P07A='1954', P08='63', P21='1', P22N='3', P23='6', P28='2', P29='24')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e951e979-1b85-4829-b172-b77a427c263a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f19be245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|urban_rural|               hh_id|\n",
      "+-----------+--------------------+\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "|          1|11101101010011066...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "sdf.createOrReplaceTempView(\"pop\")\n",
    "\n",
    "sql_df = spark.sql(\"SELECT urban_rural, hh_id FROM pop\")\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e90a9e8-d23f-43a2-9dfe-c1c55fdf3545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sql_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a4c83-8933-4903-be1f-4a3342156099",
   "metadata": {},
   "source": [
    "### Largest population. \n",
    "Which province has the largest population and whats the population?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90315a-21e4-4234-934b-52c320296ae3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  Since I'm not an SQL expert, I will add the <strong>prov_id</strong> column using DataFrame API. Otherwise, you can do all this with SQL.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca1fdca-7ebc-47dc-9310-3123f2e3e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prov_id column using DataFrame API\n",
    "sql_df2 = sql_df.withColumn('prov_id', udf(lambda x: x[0])('hh_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08151a41-96fc-429a-bc42-1fb546d1699c",
   "metadata": {},
   "source": [
    "#### DataFrame methods() which look like SQL commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04279605-595f-49f0-8712-e4d64db73d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# SQL function queries in DataFrames\n",
    "# Find the aggregate count for California by filtering \n",
    "sql_df3 = (sql_df2\n",
    "     .groupBy(\"prov_id\")\n",
    "     .agg(count(\"prov_id\").alias(\"pop\"))\n",
    "     .orderBy(\"pop\", ascending=False)\n",
    "     .first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f0698-f8d3-46b1-8ec0-6edca87bed08",
   "metadata": {},
   "source": [
    "#### Run actual SQL commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f3687-7271-4645-a5ae-a888ef99d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and register another view based on sql_df2\n",
    "sql_df2.createOrReplaceTempView(\"pop\")\n",
    "\n",
    "# Define SQL statemente to aggregate and get total population by province\n",
    "sql_statement = \"\"\"\n",
    "SELECT COUNT(prov_id) As pop, prov_id\n",
    "FROM pop\n",
    "GROUP BY prov_id\n",
    "\"\"\"\n",
    "\n",
    "# Run the SQL command \n",
    "sql_df3 = spark.sql(sql_statement)\n",
    "sql_df3.show()\n",
    "\n",
    "# We can also use SQL to select largest\n",
    "sql_df3.createOrReplaceTempView(\"pop_by_prov\")\n",
    "\n",
    "largest = spark.sql(\"SELECT MAX(pop) from pop_by_prov\")\n",
    "largest_pop = largest.collect()[0]['max(pop)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795a98b-1575-4b4b-ac58-f4ddec5fd32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_statement2 = \"\"\"\n",
    "SELECT prov_id\n",
    "FROM pop_by_prov\n",
    "WHERE pop_by_prov.pop = {}\n",
    "\"\"\".format(largest_pop)\n",
    "\n",
    "spark.sql(sql_statement2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cfa75e-39a8-4daa-8948-6b32872bec09",
   "metadata": {},
   "source": [
    "<font color='blue'>**EXERCISE-7: ANSWER THE REST OF THE ANALYSIS QUESTIONS USING SQL** </font> \n",
    "\n",
    "As a challenge, you can use SQL to run aggregations to get answers to the rest of the analysis questions. Otherwise, SQL is not the focus of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab7af1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations, you learned the basics and challenges of working with Spark RDD's in this notebook. By now, you can write simple programs uitlizing RDDs different map and reduce functions. Although you will not often work with RDDs, this understanding is crucial as RDDs, DataFrames work together and its important to know how to switch betweeen these different dataa structures depending on the use case and need. When using Spark on data science projects, you will utilize Pandas, Numpy, Spark DataFrames, Spark RDD and other Python data structures in a seamlless fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f045ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
